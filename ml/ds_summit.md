# DeepSpeed on OLCF Summit

- [DeepSpeed on OLCF Summit](#deepspeed-on-olcf-summit)
  - [Pre-requisites](#pre-requisites)
  - [Modularize](#modularize)
  - [Triton](#triton)
  - [DeepSpeed](#deepspeed)
  - [Extras](#extras)

The following describes the setup on Summit. Most of the difficulties of
setting this up are rooted from ppc64le arch.

## Pre-requisites

Base line: miniconda3-py38 are located at `/sw/aaims/summit/miniconda3`. We
created a separate environment for it:

    source /sw/aaims/summit/miniconda3/etc/profile.d/conda.sh
    conda create -p /sw/aaims/summit/deep
    conda activate /sw/aaims/summit/deep
    conda install pytorch pyarrow -c file:///sw/aaims/open-ce/condabuild-v1.2.0

Note `pytorch` is installed from a local conda repo, configured by `.condarc`:

    channels:
    - file:///sw/aaims/open-ce/condabuild-v1.2.0
    - file:///sw/sources/open-ce/conda-channel-v1.1.3
    - defaults
      
## Modularize

The module file is created as [such](deep.lua): 

    # /sw/aaims/summit/modulefiles/deep/latest.lua
    

## Triton

There is a dependency of [Triton](https://github.com/ptillet/triton) for one
of DeepSpeed operators (sparse_attn). I have
built a wheel for it. Triton built its own LLVM, and we need gcc 7.4.0 for it.
The basic steps are:

    module load gcc/7.4.0 cmake deep/latest
    git clone https://github.com/ptillet/triton
    mkdir build; cd build
    CC=/sw/summit/gcc/7.4.0/bin/gcc ccmake ..
    # we toggle PYTHON module interface
    make


For python package build, we need some additional changes on
`triton/python/setup.py`: It does two things 1) reduce parallelization,
otherwise you can run into resource contention error due to extreme high cpu
count; 2) set up proper GCC environment.


    $ git diff setup.py
    -            build_args += ["--", '-j' + str(2 * multiprocessing.cpu_count())]
    +            build_args += ["--", '-j1']
    +       os.environ["CC"]="/sw/summit/gcc/7.4.0/bin/gcc"
            env = os.environ.copy()

With these modification in place:

        python setup.py bdist_wheel

This platform specific wheel are stored in `dist` directory. 


## DeepSpeed

The following are the build status:

`DS_BUILD_OPS=1`  is used for toggling all OPS.

| Option | Status| 
| ---    | ---   |
| DS_BUILD_CPU_ADAM     | PASS |
| DS_BUILD_FUSED_ADAM   | PASS |
| DS_BUILD_FUSED_LAMB   | PASS |
| DS_BUILD_UTILS        | PASS |
| DS_BUILD_SPARSE_ATTN  | PASS |
| DS_BUILD_TRANSFORMER  | PASS |
| DS_BUILD_STOCHASTIC_TRANSFORMER | PASS |


**General fix**

we need to change the optimization flags for all ops, I am using cpu_adam as
an example:

```
diff --git a/op_builder/cpu_adam.py b/op_builder/cpu_adam.py
index 464f597..2c7d9a9 100644
--- a/op_builder/cpu_adam.py
+++ b/op_builder/cpu_adam.py
@@ -53,7 +53,7 @@ class CPUAdamBuilder(CUDAOpBuilder):
-            '-march=native',
+            '-mcpu=native',
```

**cpu_adam**

we need to comment out `csrc/includes/cpu_adam.h`

    //#include <cpuid.h>
    //#include <x86intrin.h>


**transformer**

This build will fail due to flags generated by builder. Manually, I can get rid of the error by taking out:
``-D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__
-D__CUDA_NO_HALF2_OPERATORS__``. However, these options are inserted by
pytorch cpp extension by default.

We inspect:

    import op_builder
    tr = op_uilder.transformer
    tb = tr.TransformerBuilder()
    tb.nvcc_args()

output:

    ['-O3',
    '--use_fast_math',
    '-std=c++14',
    '-gencode=arch=compute_37,code=sm_37',
    '-gencode=arch=compute_60,code=sm_60',
    '-gencode=arch=compute_70,code=sm_70',
    '-gencode=arch=compute_75,code=sm_75']

As can be seen here, the transformer build itself didn't add these compiler
options. you can get rid of these older compute by:

    TORCH_CUDA_ARCH_LIST="7.0" DS_BUILD_TRANSFORMER=1 python setup.py bdist_wheel

but this doesn't seem to address the extra flags issue.

The final fix is:

    @@ -36,6 +36,7 @@ class TransformerBuilder(CUDAOpBuilder):
                '-O3',
                '--use_fast_math',
                '-std=c++14',
    +            '-DCUDA_HAS_FP16=1',
                '-U__CUDA_NO_HALF_OPERATORS__',
                '-U__CUDA_NO_HALF_CONVERSIONS__',
                '-U__CUDA_NO_HALF2_OPERATORS__'

**update triton requirement**

Triton version is set strictly as 0.2.3, which doesn't work since we built
from source, and version is bumped up to 1.0.0. The following is the fix:

    $cat requirements/requirements-sparse_attn.txt
    triton>=0.2.3

**final wheel**

    $ TORCH_CUDA_ARCH_LIST="7.0" DS_BUILD_OPS=1 python setup.py bdist_wheel
    $ pip install dist/deepspeed-0.3.17+d88d927-cp38-cp38-linux_ppc64le.whl


## Extras

    pip install transformers datasets webdataset
    