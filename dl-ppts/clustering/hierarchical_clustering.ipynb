{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "\n",
    "## Concept\n",
    "\n",
    "HC creates a tree of clustering by iteratively group similar items together.\n",
    "As usual, similarity is defined by distance metrics such as \"euclidean distance\" (square root of sum of squares) or \"manhattan distance\" (sum of absolute value).\n",
    "\n",
    "Often, HC is associated with representation of heatmaps, **ordered** by cluster. shorter branch on the side (**dendrogram**) reflects the cluster formation timeline: the shorter branch implies this cluster is formed earlier.\n",
    "\n",
    "\n",
    "The algorithm can be stopped by:\n",
    "\n",
    "* set distance threshold: if cluster to cluster distance is beyond a threshold, then stop merging the clusters\n",
    "\n",
    "* $N$ number of cluster: the algorithm can stop if $N$ is reached. \n",
    "\n",
    "## Pros\n",
    "\n",
    "One key advantage of HC is:\n",
    "* it has no preset # of cluster numbers. It is unsupervised ML method\n",
    "* it always generate same clustering (vs. k-means which depends on the initial centroid placement)\n",
    "\n",
    "\n",
    "\n",
    "## Cons\n",
    "\n",
    "* It is slower than k-means, especially for large dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Coming."
   ]
  }
 ]
}