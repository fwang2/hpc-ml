{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NN\n",
    "\n",
    "So far, a network takes input, the generate outputs, only the error is propagated back.\n",
    "RNN network takes inputs, at the same time, the output from previous state are fed back as another input. It enabled the RNN to retain certain type of **memory**, which make it useful for:\n",
    "* time series prediction\n",
    "* language modeling, sentimental analysis\n",
    "* translation, speech recognition ...\n",
    "\n",
    "\n",
    "## pytorch API\n",
    "\n",
    "The interface is friendly:\n",
    "\n",
    "The hidden_size is the output size\n",
    "\n",
    "```\n",
    "cell = nn.RNN(input_size = 4, hidden_size = 2, batch_first = True)\n",
    "cell = nn.GRU(input_size = 4, hidden_size = 2, batch_first = True)\n",
    "cell = nn.LSTM(input_size = 4, hidden_size = 2, batch_first = True)\n",
    "```\n",
    "\n",
    "Once a cell is defined:\n",
    "\n",
    "```\n",
    "out, hidden = cell(inputs, hidden)\n",
    "```\n",
    "\n",
    "So the cell output has two parts: the output itself, and hidden output.\n",
    "\n",
    "## Example: feed letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# One hot encoding for each char in 'hello'\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5\n",
    "cell = nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n",
    "\n",
    "# (num_layers * num_directions, batch, hidden_size) whether batch_first=True or False\n",
    "# tensor([[[-1.4432,  1.7785]]])\n",
    "hidden = Variable(torch.randn(1, 1, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: feed one letter at a time, input shape (1, 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "input= tensor([[[1., 0., 0., 0.]]])\ninput_size= torch.Size([1, 1, 4]) outsize= torch.Size([1, 1, 2]) out_value= tensor([[[-0.6660,  0.6075]]])\ninput= tensor([[[0., 1., 0., 0.]]])\ninput_size= torch.Size([1, 1, 4]) outsize= torch.Size([1, 1, 2]) out_value= tensor([[[-0.4775,  0.7268]]])\ninput= tensor([[[0., 0., 1., 0.]]])\ninput_size= torch.Size([1, 1, 4]) outsize= torch.Size([1, 1, 2]) out_value= tensor([[[-0.4449,  0.1278]]])\ninput= tensor([[[0., 0., 1., 0.]]])\ninput_size= torch.Size([1, 1, 4]) outsize= torch.Size([1, 1, 2]) out_value= tensor([[[-0.2878, -0.1194]]])\ninput= tensor([[[0., 0., 0., 1.]]])\ninput_size= torch.Size([1, 1, 4]) outsize= torch.Size([1, 1, 2]) out_value= tensor([[[0.0518, 0.4729]]])\n"
    }
   ],
   "source": [
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "inputs = Variable(torch.Tensor([h, e, l, l, o]))\n",
    "for one in inputs:\n",
    "    one = one.view(1, 1, -1)\n",
    "    print(\"input=\", one)\n",
    "    # Input: (batch, seq_len, input_size) when batch_first=True\n",
    "    out, hidden = cell(one, hidden)\n",
    "    print(\"input_size=\", one.size(), \"outsize=\", out.size(), \"out_value=\", out.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: feed one \"sequence\" at a time, input shape (1, 5, 4)\n",
    "\n",
    "The second number 5 is the length of the letter, \"hello\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "input_size= torch.Size([1, 1, 4]) outsize= torch.Size([1, 5, 2]) out_value=\n tensor([[[-0.6630,  0.6087],\n         [-0.4768,  0.7275],\n         [-0.4449,  0.1284],\n         [-0.2879, -0.1192],\n         [ 0.0517,  0.4729]]])\n"
    }
   ],
   "source": [
    "inputs = Variable(torch.Tensor([h, e, l, l, o]))\n",
    "# We can do the whole at once\n",
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "inputs = inputs.view(1, 5, -1)\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"input_size=\", one.size(), \"outsize=\", out.size(), \"out_value=\\n\", out.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: feed multiple words or \"batch\" at a time, input shape (3, 5, 4)\n",
    "\n",
    "If we have three words to feed, the first number is for that.\n",
    "Of course, we are limited to encode just a few letters here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "input_size= torch.Size([1, 1, 4]) outsize= torch.Size([3, 5, 2]) out_value=\n tensor([[[ 0.2906,  0.8383],\n         [-0.0809,  0.4115],\n         [-0.3715,  0.3934],\n         [-0.2620,  0.2893],\n         [ 0.5988,  0.2444]],\n\n        [[ 0.4730,  0.1992],\n         [ 0.3435,  0.4745],\n         [-0.5093,  0.5346],\n         [-0.1811,  0.2702],\n         [-0.3563,  0.3304]],\n\n        [[-0.5692,  0.5406],\n         [-0.1550,  0.2497],\n         [ 0.0040,  0.1241],\n         [-0.0872,  0.1561],\n         [-0.4086,  0.3393]]])\n"
    }
   ],
   "source": [
    "# hidden : (num_layers * num_directions, batch, hidden_size) whether batch_first=True or False\n",
    "hidden = Variable(torch.randn(1, 3, 2))\n",
    "\n",
    "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
    "# 3 batches 'hello', 'eolll', 'lleel'\n",
    "# rank = (3, 5, 4)\n",
    "inputs = Variable(torch.Tensor([[h, e, l, l, o],\n",
    "                                [e, o, l, l, l],\n",
    "                                [l, l, e, e, l]]))\n",
    "\n",
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "# B x S x I\n",
    "out, hidden = cell(inputs, hidden)\n",
    "\n",
    "print(\"input_size=\", one.size(), \"outsize=\", out.size(), \"out_value=\\n\", out.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RNN to predict next letter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "RNN(\n  (rnn): RNN(5, 5, batch_first=True)\n)\nepoch: 1, loss: 1.693\nPredicted string:  llllll\nepoch: 2, loss: 1.523\nPredicted string:  llllll\nepoch: 3, loss: 1.393\nPredicted string:  llllll\nepoch: 4, loss: 1.263\nPredicted string:  llllll\nepoch: 5, loss: 1.146\nPredicted string:  llllll\nepoch: 6, loss: 1.055\nPredicted string:  lhelll\nepoch: 7, loss: 1.002\nPredicted string:  ihelll\nepoch: 8, loss: 0.965\nPredicted string:  ihelll\nepoch: 9, loss: 0.913\nPredicted string:  ihelll\nepoch: 10, loss: 0.879\nPredicted string:  ihelll\nepoch: 11, loss: 0.840\nPredicted string:  ihelll\nepoch: 12, loss: 0.805\nPredicted string:  ihello\nepoch: 13, loss: 0.779\nPredicted string:  ihello\nepoch: 14, loss: 0.758\nPredicted string:  ihello\nepoch: 15, loss: 0.738\nPredicted string:  ihello\nepoch: 16, loss: 0.717\nPredicted string:  ihello\nepoch: 17, loss: 0.694\nPredicted string:  ihello\nepoch: 18, loss: 0.667\nPredicted string:  ihelll\nepoch: 19, loss: 0.643\nPredicted string:  ihelll\nepoch: 20, loss: 0.647\nPredicted string:  ihelll\nepoch: 21, loss: 0.628\nPredicted string:  ihelll\nepoch: 22, loss: 0.607\nPredicted string:  ihelll\nepoch: 23, loss: 0.600\nPredicted string:  ihelll\nepoch: 24, loss: 0.596\nPredicted string:  ihello\nepoch: 25, loss: 0.591\nPredicted string:  ihello\nepoch: 26, loss: 0.583\nPredicted string:  ihello\nepoch: 27, loss: 0.573\nPredicted string:  ihello\nepoch: 28, loss: 0.562\nPredicted string:  ihello\nepoch: 29, loss: 0.550\nPredicted string:  ihello\nepoch: 30, loss: 0.540\nPredicted string:  ihello\nepoch: 31, loss: 0.527\nPredicted string:  ihello\nepoch: 32, loss: 0.524\nPredicted string:  ihello\nepoch: 33, loss: 0.530\nPredicted string:  ihello\nepoch: 34, loss: 0.519\nPredicted string:  ihello\nepoch: 35, loss: 0.507\nPredicted string:  ihello\nepoch: 36, loss: 0.503\nPredicted string:  ihello\nepoch: 37, loss: 0.503\nPredicted string:  ihello\nepoch: 38, loss: 0.500\nPredicted string:  ihello\nepoch: 39, loss: 0.496\nPredicted string:  ihello\nepoch: 40, loss: 0.494\nPredicted string:  ihello\nepoch: 41, loss: 0.493\nPredicted string:  ihello\nepoch: 42, loss: 0.492\nPredicted string:  ihello\nepoch: 43, loss: 0.488\nPredicted string:  ihello\nepoch: 44, loss: 0.484\nPredicted string:  ihello\nepoch: 45, loss: 0.481\nPredicted string:  ihello\nepoch: 46, loss: 0.481\nPredicted string:  ihello\nepoch: 47, loss: 0.480\nPredicted string:  ihello\nepoch: 48, loss: 0.477\nPredicted string:  ihello\nepoch: 49, loss: 0.476\nPredicted string:  ihello\nepoch: 50, loss: 0.476\nPredicted string:  ihello\nepoch: 51, loss: 0.475\nPredicted string:  ihello\nepoch: 52, loss: 0.473\nPredicted string:  ihello\nepoch: 53, loss: 0.472\nPredicted string:  ihello\nepoch: 54, loss: 0.472\nPredicted string:  ihello\nepoch: 55, loss: 0.471\nPredicted string:  ihello\nepoch: 56, loss: 0.469\nPredicted string:  ihello\nepoch: 57, loss: 0.469\nPredicted string:  ihello\nepoch: 58, loss: 0.469\nPredicted string:  ihello\nepoch: 59, loss: 0.468\nPredicted string:  ihello\nepoch: 60, loss: 0.467\nPredicted string:  ihello\nepoch: 61, loss: 0.467\nPredicted string:  ihello\nepoch: 62, loss: 0.467\nPredicted string:  ihello\nepoch: 63, loss: 0.466\nPredicted string:  ihello\nepoch: 64, loss: 0.466\nPredicted string:  ihello\nepoch: 65, loss: 0.466\nPredicted string:  ihello\nepoch: 66, loss: 0.465\nPredicted string:  ihello\nepoch: 67, loss: 0.464\nPredicted string:  ihello\nepoch: 68, loss: 0.464\nPredicted string:  ihello\nepoch: 69, loss: 0.464\nPredicted string:  ihello\nepoch: 70, loss: 0.463\nPredicted string:  ihello\nepoch: 71, loss: 0.463\nPredicted string:  ihello\nepoch: 72, loss: 0.463\nPredicted string:  ihello\nepoch: 73, loss: 0.463\nPredicted string:  ihello\nepoch: 74, loss: 0.462\nPredicted string:  ihello\nepoch: 75, loss: 0.462\nPredicted string:  ihello\nepoch: 76, loss: 0.462\nPredicted string:  ihello\nepoch: 77, loss: 0.462\nPredicted string:  ihello\nepoch: 78, loss: 0.462\nPredicted string:  ihello\nepoch: 79, loss: 0.461\nPredicted string:  ihello\nepoch: 80, loss: 0.461\nPredicted string:  ihello\nepoch: 81, loss: 0.461\nPredicted string:  ihello\nepoch: 82, loss: 0.461\nPredicted string:  ihello\nepoch: 83, loss: 0.460\nPredicted string:  ihello\nepoch: 84, loss: 0.460\nPredicted string:  ihello\nepoch: 85, loss: 0.460\nPredicted string:  ihello\nepoch: 86, loss: 0.460\nPredicted string:  ihello\nepoch: 87, loss: 0.460\nPredicted string:  ihello\nepoch: 88, loss: 0.460\nPredicted string:  ihello\nepoch: 89, loss: 0.459\nPredicted string:  ihello\nepoch: 90, loss: 0.459\nPredicted string:  ihello\nepoch: 91, loss: 0.459\nPredicted string:  ihello\nepoch: 92, loss: 0.459\nPredicted string:  ihello\nepoch: 93, loss: 0.459\nPredicted string:  ihello\nepoch: 94, loss: 0.459\nPredicted string:  ihello\nepoch: 95, loss: 0.459\nPredicted string:  ihello\nepoch: 96, loss: 0.458\nPredicted string:  ihello\nepoch: 97, loss: 0.458\nPredicted string:  ihello\nepoch: 98, loss: 0.458\nPredicted string:  ihello\nepoch: 99, loss: 0.458\nPredicted string:  ihello\nepoch: 100, loss: 0.458\nPredicted string:  ihello\nLearning finished!\n"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "\n",
    "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
    "\n",
    "# As we have one batch of samples, we will change them to variables only once\n",
    "inputs = Variable(torch.Tensor(x_one_hot))\n",
    "labels = Variable(torch.LongTensor(y_data))\n",
    "\n",
    "num_classes = 5\n",
    "input_size = 5  # one-hot size\n",
    "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 6  # |ihello| == 6\n",
    "num_layers = 1  # one-layer rnn\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=5, hidden_size=5, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size) for batch_first=True\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "\n",
    "        # Reshape input\n",
    "        x.view(x.size(0), self.sequence_length, self.input_size)\n",
    "\n",
    "        # Propagate input through RNN\n",
    "        # Input: (batch, seq_len, input_size)\n",
    "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "        out, _ = self.rnn(x, h_0)\n",
    "        return out.view(-1, num_classes)\n",
    "\n",
    "\n",
    "# Instantiate RNN model\n",
    "rnn = RNN(num_classes, input_size, hidden_size, num_layers)\n",
    "print(rnn)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    outputs = rnn(inputs)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, idx = outputs.max(1)\n",
    "    idx = idx.data.numpy()\n",
    "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
    "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.item()))\n",
    "    print(\"Predicted string: \", ''.join(result_str))\n",
    "\n",
    "print(\"Learning finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}