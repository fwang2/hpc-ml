{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Environment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note: that graphviz needs to be binary installed, the package\n",
    "needs \"dot\" command available in the path.\n",
    "\n",
    "```\n",
    "!pip install graphviz torchviz\n",
    "```\n",
    "\n",
    "On MBP14, this is tested under \"torch-gpu-py39\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 7.1.0 (20230121.1956)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"62pt\" height=\"39pt\"\n",
       " viewBox=\"0.00 0.00 62.00 39.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 35)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-35 58,-35 58,4 -4,4\"/>\n",
       "<!-- 140242098172448 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140242098172448</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"54,-31 0,-31 0,0 54,0 54,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f8cf1934c70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tensorboard as tb\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "    \n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from torchviz import make_dot\n",
    "v = torch.tensor(1.0, requires_grad=True)\n",
    "make_dot(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch: Tensors \n",
    "\n",
    "* A **scalar** has zero dimensions\n",
    "* A **vector** has 1 dimension\n",
    "* A **matrix** has 2 dimensions\n",
    "* A **tensor** has 3 or more dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "scalar = torch.tensor(3.14)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "matrix = torch.ones((2,3), dtype=torch.float)\n",
    "tensor = torch.randn((2,3,4), dtype=torch.float)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scalar.size(), scalar.shape)\n",
    "print(tensor.size(), tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a different view\n",
    "\n",
    "same_matrix = matrix.view(1,6)\n",
    "print(matrix)\n",
    "print(same_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "\"Deep Learning with PyTorch\" Step by Step Guide\n",
    "published around 2021, https://github.com/dvgodoy/PyTorchStepByStep\n",
    "\n",
    "\n",
    "\"Deep Learning with PyTorch\", by Eli Stevens, 2020\n",
    "https://github.com/deep-learning-with-pytorch/dlwpt-code\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Visualizing Gradient Descent\n",
    "\n",
    "\n",
    "Github code: https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter01.ipynb\n",
    "\n",
    "\n",
    "This book uses: \n",
    "\n",
    "$$ y = b + wx + \\epsilon $$\n",
    "\n",
    "that is, feature ($x$) to predict a label $y$. The **parameter** $b$ is the bias, which tell us the expected value of $y$ when $x=0$; **parameter** $w$ or the weights tell us how much $y$ increaes on average, if we increase $x$ by one unit. The last term is there to account for inherent **noise**, that is the error we can't get rid of.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prep the synthetic data\n",
    "\n",
    "1) generate N number of random values (0,1)\n",
    "2) add each with some noise from \"standard normal\"\n",
    "3) split train and val by 80% and 20% (indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data generation\n",
    "true_b = 1\n",
    "true_w = 2\n",
    "N = 100\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N, 1) # random value between 0 and 1\n",
    "\n",
    "epsilon = (0.1 * np.random.randn(N,1)) # from normal distribution\n",
    "\n",
    "y = true_b + true_w * x + epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# shuffles the indices\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# use first 80 random indices for train\n",
    "train_idx = idx[:int(N * 0.8)]\n",
    "\n",
    "# use the remaining indices for validation\n",
    "\n",
    "val_idx = idx[int(N*0.8):]\n",
    "\n",
    "# generates train and validataion sets\n",
    "\n",
    "x_train , y_train = x[train_idx], y[train_idx]\n",
    "x_val , y_val = x[val_idx], y[val_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def figure1(x_train, y_train, x_val, y_val):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    ax[0].scatter(x_train, y_train)\n",
    "    ax[0].set_xlabel('x')\n",
    "    ax[0].set_ylabel('y')\n",
    "    ax[0].set_ylim([0, 3.1])\n",
    "    ax[0].set_title('Generated Data - Train')\n",
    "\n",
    "    ax[1].scatter(x_val, y_val, c='r')\n",
    "    ax[1].set_xlabel('x')\n",
    "    ax[1].set_ylabel('y')\n",
    "    ax[1].set_ylim([0, 3.1])\n",
    "    ax[1].set_title('Generated Data - Validation')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1(x_train,y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# step 0 random init\n",
    "# initialize the parameters \"b\" and \"w\" randomly\n",
    "\n",
    "np.random.seed(42)\n",
    "b = np.random.randn(1)\n",
    "w = np.random.randn(1)\n",
    "print(b,w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: compute model predictions\n",
    "\n",
    "forward pass - compute the model predition using current value of w anb b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = b + w * x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### step 2: compute the loss\n",
    "\n",
    "for a regression program, the loss is given by Mean Squared Error (MSE), that is, the average of all squared erros.\n",
    "\n",
    "in the code below, we are using all data points of the training set to compute the loss, so $n = N = 80$, so this is batch gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = (yhat - y_train)\n",
    "\n",
    "# we now compute MSE\n",
    "\n",
    "loss = (error **2).mean()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3: Compute the gradient\n",
    "\n",
    "a **gradient** is a **partial derivative**. It tells you how much a **given quantity changes**, when you slightly vary the **other quantity**. Here the **given quantity** is usually the loss, the **other quanity** is some parameter under consideration, in this case, that is $b$ and $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_grad = 2 * error.mean()\n",
    "w_grad = 2 * (x_train * error).mean()\n",
    "print(b_grad, w_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4: update parameters\n",
    "\n",
    "in this step, we use gradient to update the parameters. we use negative sign of the gradient for the update.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1 # learning rate, \"eta\" in Greek letter\n",
    "print(b, w)\n",
    "\n",
    "# update parameter\n",
    "\n",
    "b = b - lr * b_grad\n",
    "w = w - lr * w_grad\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 5: repeat\n",
    "\n",
    "we now go back to step 1 and restart the process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Epoch\n",
    "\n",
    "A epoch is complete whenever every point in the training set (N) has already been use in all steps: forward pass, computing the loss, computing the gradient, and update the parameters.\n",
    "\n",
    "The number of **update** varies:\n",
    "\n",
    "* for batch ($n=N$), one epoch is the same as **one update**\n",
    "* for stochastic ($n=1$), one epoch means $N$ updates\n",
    "* for mini-batch (of size $n$), one epoch has $\\frac{N}{n}$ updates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss vs. Error\n",
    "\n",
    "The **error** is the difference between the **actual value** and **predicted value** computed for a single data point. For for $i-th$ data point:\n",
    "\n",
    "$$\\text{error}_i = \\hat{y_i} - y_i$$\n",
    "\n",
    "The **losss**, on the other hand, it some sort of **aggregation** of errors for a set of data points.\n",
    "\n",
    "* If we use **all points** in the training dataset, $n=N$ to compute the loss, we are performing a **batch gradient descent**.\n",
    "\n",
    "* If we use a **single point** ($n=1$) each time, it will be stochastic gradient descent.\n",
    "\n",
    "* Anything else in between 1 and $N$, that is **mini-batch gradient descent**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Metrics for Regression: Mean Squared Error (MSE)\n",
    "\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} {error_i}^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
