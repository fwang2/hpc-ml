{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax activation\n",
    "\n",
    "This note uses MNIST dataset to illustrate this activation function.\n",
    "\n",
    "The input is $(N, M)$, where $N$ is the number of samples, $M$ is the number of features.\n",
    "The output is one of 0 to 9. This requires the weight to be of shape $(M, 10)$. Since this falls into the question of \"Which one of the output is best\"? [Activation functions](activation_functions.ipynb) discussed why softmax is better than sigmoid in this case.\n",
    "\n",
    "\n",
    "General definition of softmax:\n",
    "\n",
    "$$ \\sigma(z_j) = \\frac{e^{z_j}}{\\sum_{i=1}^{K}e^{z_i}}$$\n",
    "\n",
    "Here, there are a total of $K$ outputs.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.54279046e-02 1.03952403e-04 2.08793983e-03 2.08793983e-03\n",
      " 1.13997652e-01 8.42335048e-01 2.08793983e-03 1.54279046e-02\n",
      " 7.68110139e-04 5.67560891e-03]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "np.random.seed(0)\n",
    "output = np.random.randint(0,10, size=10)\n",
    "sm_output = softmax(output)\n",
    "print(sm_output)\n",
    "print(np.sum(sm_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Logits and Cross Entropy\n",
    "\n",
    "The output from linear regression is known as logits, they are the inputs to the softmax. Cross entropy as defined here, can compute the differences between two distributions, $\\hat{y}$ and $y$.\n",
    "\n",
    "The sum of these differences forms the Loss function.mro\n",
    "\n",
    "![loss function](../figs/cross-entropy.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1 = 0.35667494393873245\n",
      "loss2 = 1.6094379124341003\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "def cross_entropy(Y, Y_pred):\n",
    "    \"\"\" Note: there is no softmax applied to Y_pred here \"\"\"\n",
    "    return np.sum(-Y * np.log(Y_pred))\n",
    "\n",
    "\n",
    "# one-hot encoding\n",
    "Y = np.array([0, 1, 0])\n",
    "\n",
    "# This will NOT work ...\n",
    "# Y_pred1 = np.array([.2, .7, 0])\n",
    "Y_pred1 = np.array([.1, .7, .1])\n",
    "Y_pred2 = np.array([.7, .2, .1])\n",
    "\n",
    "print(f\"loss1 = {cross_entropy(Y, Y_pred1)}\") # good prediction\n",
    "print(f\"loss2 = {cross_entropy(Y, Y_pred2)}\") # bad prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch loss1=0.4076060354709625\n",
      "PyTorch loss2=6.009174346923828\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Softmax + CrossEntropy\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# input is class, not one-hot\n",
    "# for example: if there are three target value\n",
    "# it should be either 0, 1, or 2.\n",
    "# in this case, we define it as 1\n",
    "Y = Variable(torch.LongTensor([1]), requires_grad=False)\n",
    "\n",
    "# Input are logits, not softmax \n",
    "Y_pred1 = Variable(torch.Tensor([[1.0, 3.0, 2.0]]))\n",
    "Y_pred2 = Variable(torch.Tensor([[7.0, 1.0, 2.0]]))\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "print(f\"PyTorch loss1={l1.data}\")\n",
    "print(f\"PyTorch loss2={l2.data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Loss \n",
    "\n",
    "PyTorch supports prediction and target in batch: that is, multple prediction correspond to multiple targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch loss1=0.4966353178024292\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Softmax + CrossEntropy\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# input is class, not one-hot\n",
    "# for example: if there are three target value\n",
    "# it should be either 0, 1, or 2.\n",
    "# in this case, we define it as 1\n",
    "Y = Variable(torch.LongTensor([2, 0, 1]), requires_grad=False)\n",
    "\n",
    "# Input are logits, not softmax \n",
    "Y_pred1 = Variable(torch.Tensor([\n",
    "    [0.1, 0.2, 0.9],\n",
    "    [1.1, 0.1, 0.2],\n",
    "    [0.2, 2.1, 0.1]\n",
    "    ])) # good predition\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "print(f\"PyTorch loss1={l1.data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MNIST Model on cpu\n",
      "============================================\n",
      "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.296754\n",
      "Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 2.296579\n",
      "Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 2.303263\n",
      "Train Epoch: 1 | Batch Status: 1920/60000 (3%) | Loss: 2.297008\n",
      "Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 2.301572\n",
      "Train Epoch: 1 | Batch Status: 3200/60000 (5%) | Loss: 2.293637\n",
      "Train Epoch: 1 | Batch Status: 3840/60000 (6%) | Loss: 2.295901\n",
      "Train Epoch: 1 | Batch Status: 4480/60000 (7%) | Loss: 2.286935\n",
      "Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 2.302372\n",
      "Train Epoch: 1 | Batch Status: 5760/60000 (10%) | Loss: 2.300096\n",
      "Train Epoch: 1 | Batch Status: 6400/60000 (11%) | Loss: 2.299518\n",
      "Train Epoch: 1 | Batch Status: 7040/60000 (12%) | Loss: 2.293327\n",
      "Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 2.298049\n",
      "Train Epoch: 1 | Batch Status: 8320/60000 (14%) | Loss: 2.291863\n",
      "Train Epoch: 1 | Batch Status: 8960/60000 (15%) | Loss: 2.301373\n",
      "Train Epoch: 1 | Batch Status: 9600/60000 (16%) | Loss: 2.293327\n",
      "Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 2.305115\n",
      "Train Epoch: 1 | Batch Status: 10880/60000 (18%) | Loss: 2.300747\n",
      "Train Epoch: 1 | Batch Status: 11520/60000 (19%) | Loss: 2.304473\n",
      "Train Epoch: 1 | Batch Status: 12160/60000 (20%) | Loss: 2.291578\n",
      "Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 2.287332\n",
      "Train Epoch: 1 | Batch Status: 13440/60000 (22%) | Loss: 2.296753\n",
      "Train Epoch: 1 | Batch Status: 14080/60000 (23%) | Loss: 2.293263\n",
      "Train Epoch: 1 | Batch Status: 14720/60000 (25%) | Loss: 2.297856\n",
      "Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 2.299801\n",
      "Train Epoch: 1 | Batch Status: 16000/60000 (27%) | Loss: 2.285758\n",
      "Train Epoch: 1 | Batch Status: 16640/60000 (28%) | Loss: 2.298857\n",
      "Train Epoch: 1 | Batch Status: 17280/60000 (29%) | Loss: 2.299472\n",
      "Train Epoch: 1 | Batch Status: 17920/60000 (30%) | Loss: 2.288281\n",
      "Train Epoch: 1 | Batch Status: 18560/60000 (31%) | Loss: 2.291917\n",
      "Train Epoch: 1 | Batch Status: 19200/60000 (32%) | Loss: 2.292340\n",
      "Train Epoch: 1 | Batch Status: 19840/60000 (33%) | Loss: 2.288103\n",
      "Train Epoch: 1 | Batch Status: 20480/60000 (34%) | Loss: 2.292995\n",
      "Train Epoch: 1 | Batch Status: 21120/60000 (35%) | Loss: 2.295974\n",
      "Train Epoch: 1 | Batch Status: 21760/60000 (36%) | Loss: 2.287154\n",
      "Train Epoch: 1 | Batch Status: 22400/60000 (37%) | Loss: 2.287747\n",
      "Train Epoch: 1 | Batch Status: 23040/60000 (38%) | Loss: 2.282001\n",
      "Train Epoch: 1 | Batch Status: 23680/60000 (39%) | Loss: 2.288192\n",
      "Train Epoch: 1 | Batch Status: 24320/60000 (41%) | Loss: 2.281674\n",
      "Train Epoch: 1 | Batch Status: 24960/60000 (42%) | Loss: 2.281035\n",
      "Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 2.286396\n",
      "Train Epoch: 1 | Batch Status: 26240/60000 (44%) | Loss: 2.279931\n",
      "Train Epoch: 1 | Batch Status: 26880/60000 (45%) | Loss: 2.280185\n",
      "Train Epoch: 1 | Batch Status: 27520/60000 (46%) | Loss: 2.288268\n",
      "Train Epoch: 1 | Batch Status: 28160/60000 (47%) | Loss: 2.276734\n",
      "Train Epoch: 1 | Batch Status: 28800/60000 (48%) | Loss: 2.279650\n",
      "Train Epoch: 1 | Batch Status: 29440/60000 (49%) | Loss: 2.274508\n",
      "Train Epoch: 1 | Batch Status: 30080/60000 (50%) | Loss: 2.283664\n",
      "Train Epoch: 1 | Batch Status: 30720/60000 (51%) | Loss: 2.278988\n",
      "Train Epoch: 1 | Batch Status: 31360/60000 (52%) | Loss: 2.259857\n",
      "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 2.267518\n",
      "Train Epoch: 1 | Batch Status: 32640/60000 (54%) | Loss: 2.268929\n",
      "Train Epoch: 1 | Batch Status: 33280/60000 (55%) | Loss: 2.264883\n",
      "Train Epoch: 1 | Batch Status: 33920/60000 (57%) | Loss: 2.261382\n",
      "Train Epoch: 1 | Batch Status: 34560/60000 (58%) | Loss: 2.269572\n",
      "Train Epoch: 1 | Batch Status: 35200/60000 (59%) | Loss: 2.255080\n",
      "Train Epoch: 1 | Batch Status: 35840/60000 (60%) | Loss: 2.253910\n",
      "Train Epoch: 1 | Batch Status: 36480/60000 (61%) | Loss: 2.244739\n",
      "Train Epoch: 1 | Batch Status: 37120/60000 (62%) | Loss: 2.255109\n",
      "Train Epoch: 1 | Batch Status: 37760/60000 (63%) | Loss: 2.256369\n",
      "Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 2.256891\n",
      "Train Epoch: 1 | Batch Status: 39040/60000 (65%) | Loss: 2.258150\n",
      "Train Epoch: 1 | Batch Status: 39680/60000 (66%) | Loss: 2.230414\n",
      "Train Epoch: 1 | Batch Status: 40320/60000 (67%) | Loss: 2.239524\n",
      "Train Epoch: 1 | Batch Status: 40960/60000 (68%) | Loss: 2.236597\n",
      "Train Epoch: 1 | Batch Status: 41600/60000 (69%) | Loss: 2.222728\n",
      "Train Epoch: 1 | Batch Status: 42240/60000 (70%) | Loss: 2.227886\n",
      "Train Epoch: 1 | Batch Status: 42880/60000 (71%) | Loss: 2.205230\n",
      "Train Epoch: 1 | Batch Status: 43520/60000 (72%) | Loss: 2.219610\n",
      "Train Epoch: 1 | Batch Status: 44160/60000 (74%) | Loss: 2.198335\n",
      "Train Epoch: 1 | Batch Status: 44800/60000 (75%) | Loss: 2.198237\n",
      "Train Epoch: 1 | Batch Status: 45440/60000 (76%) | Loss: 2.172526\n",
      "Train Epoch: 1 | Batch Status: 46080/60000 (77%) | Loss: 2.185679\n",
      "Train Epoch: 1 | Batch Status: 46720/60000 (78%) | Loss: 2.140243\n",
      "Train Epoch: 1 | Batch Status: 47360/60000 (79%) | Loss: 2.150813\n",
      "Train Epoch: 1 | Batch Status: 48000/60000 (80%) | Loss: 2.136705\n",
      "Train Epoch: 1 | Batch Status: 48640/60000 (81%) | Loss: 2.114789\n",
      "Train Epoch: 1 | Batch Status: 49280/60000 (82%) | Loss: 2.026657\n",
      "Train Epoch: 1 | Batch Status: 49920/60000 (83%) | Loss: 2.063662\n",
      "Train Epoch: 1 | Batch Status: 50560/60000 (84%) | Loss: 1.963524\n",
      "Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 2.025182\n",
      "Train Epoch: 1 | Batch Status: 51840/60000 (86%) | Loss: 2.021616\n",
      "Train Epoch: 1 | Batch Status: 52480/60000 (87%) | Loss: 2.009329\n",
      "Train Epoch: 1 | Batch Status: 53120/60000 (88%) | Loss: 1.975234\n",
      "Train Epoch: 1 | Batch Status: 53760/60000 (90%) | Loss: 1.965269\n",
      "Train Epoch: 1 | Batch Status: 54400/60000 (91%) | Loss: 1.857516\n",
      "Train Epoch: 1 | Batch Status: 55040/60000 (92%) | Loss: 1.873605\n",
      "Train Epoch: 1 | Batch Status: 55680/60000 (93%) | Loss: 1.899332\n",
      "Train Epoch: 1 | Batch Status: 56320/60000 (94%) | Loss: 1.758082\n",
      "Train Epoch: 1 | Batch Status: 56960/60000 (95%) | Loss: 1.742574\n",
      "Train Epoch: 1 | Batch Status: 57600/60000 (96%) | Loss: 1.677520\n",
      "Train Epoch: 1 | Batch Status: 58240/60000 (97%) | Loss: 1.657828\n",
      "Train Epoch: 1 | Batch Status: 58880/60000 (98%) | Loss: 1.600177\n",
      "Train Epoch: 1 | Batch Status: 59520/60000 (99%) | Loss: 1.535530\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0227, Accuracy: 6215/10000 (62%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 1.514128\n",
      "Train Epoch: 2 | Batch Status: 640/60000 (1%) | Loss: 1.562322\n",
      "Train Epoch: 2 | Batch Status: 1280/60000 (2%) | Loss: 1.290895\n",
      "Train Epoch: 2 | Batch Status: 1920/60000 (3%) | Loss: 1.265123\n",
      "Train Epoch: 2 | Batch Status: 2560/60000 (4%) | Loss: 1.279893\n",
      "Train Epoch: 2 | Batch Status: 3200/60000 (5%) | Loss: 1.305259\n",
      "Train Epoch: 2 | Batch Status: 3840/60000 (6%) | Loss: 1.243314\n",
      "Train Epoch: 2 | Batch Status: 4480/60000 (7%) | Loss: 1.121141\n",
      "Train Epoch: 2 | Batch Status: 5120/60000 (9%) | Loss: 1.162419\n",
      "Train Epoch: 2 | Batch Status: 5760/60000 (10%) | Loss: 0.924492\n",
      "Train Epoch: 2 | Batch Status: 6400/60000 (11%) | Loss: 0.966114\n",
      "Train Epoch: 2 | Batch Status: 7040/60000 (12%) | Loss: 1.055625\n",
      "Train Epoch: 2 | Batch Status: 7680/60000 (13%) | Loss: 0.980857\n",
      "Train Epoch: 2 | Batch Status: 8320/60000 (14%) | Loss: 1.055209\n",
      "Train Epoch: 2 | Batch Status: 8960/60000 (15%) | Loss: 0.902967\n",
      "Train Epoch: 2 | Batch Status: 9600/60000 (16%) | Loss: 0.867108\n",
      "Train Epoch: 2 | Batch Status: 10240/60000 (17%) | Loss: 0.817507\n",
      "Train Epoch: 2 | Batch Status: 10880/60000 (18%) | Loss: 0.634652\n",
      "Train Epoch: 2 | Batch Status: 11520/60000 (19%) | Loss: 0.907957\n",
      "Train Epoch: 2 | Batch Status: 12160/60000 (20%) | Loss: 0.773588\n",
      "Train Epoch: 2 | Batch Status: 12800/60000 (21%) | Loss: 0.827737\n",
      "Train Epoch: 2 | Batch Status: 13440/60000 (22%) | Loss: 0.671547\n",
      "Train Epoch: 2 | Batch Status: 14080/60000 (23%) | Loss: 1.070260\n",
      "Train Epoch: 2 | Batch Status: 14720/60000 (25%) | Loss: 0.847733\n",
      "Train Epoch: 2 | Batch Status: 15360/60000 (26%) | Loss: 0.944874\n",
      "Train Epoch: 2 | Batch Status: 16000/60000 (27%) | Loss: 0.639378\n",
      "Train Epoch: 2 | Batch Status: 16640/60000 (28%) | Loss: 0.749774\n",
      "Train Epoch: 2 | Batch Status: 17280/60000 (29%) | Loss: 0.610368\n",
      "Train Epoch: 2 | Batch Status: 17920/60000 (30%) | Loss: 0.477235\n",
      "Train Epoch: 2 | Batch Status: 18560/60000 (31%) | Loss: 0.841288\n",
      "Train Epoch: 2 | Batch Status: 19200/60000 (32%) | Loss: 1.052060\n",
      "Train Epoch: 2 | Batch Status: 19840/60000 (33%) | Loss: 0.531717\n",
      "Train Epoch: 2 | Batch Status: 20480/60000 (34%) | Loss: 0.689308\n",
      "Train Epoch: 2 | Batch Status: 21120/60000 (35%) | Loss: 0.391525\n",
      "Train Epoch: 2 | Batch Status: 21760/60000 (36%) | Loss: 1.023205\n",
      "Train Epoch: 2 | Batch Status: 22400/60000 (37%) | Loss: 0.602890\n",
      "Train Epoch: 2 | Batch Status: 23040/60000 (38%) | Loss: 0.582816\n",
      "Train Epoch: 2 | Batch Status: 23680/60000 (39%) | Loss: 0.624108\n",
      "Train Epoch: 2 | Batch Status: 24320/60000 (41%) | Loss: 0.545432\n",
      "Train Epoch: 2 | Batch Status: 24960/60000 (42%) | Loss: 0.650256\n",
      "Train Epoch: 2 | Batch Status: 25600/60000 (43%) | Loss: 0.633026\n",
      "Train Epoch: 2 | Batch Status: 26240/60000 (44%) | Loss: 0.682061\n",
      "Train Epoch: 2 | Batch Status: 26880/60000 (45%) | Loss: 0.665262\n",
      "Train Epoch: 2 | Batch Status: 27520/60000 (46%) | Loss: 0.718751\n",
      "Train Epoch: 2 | Batch Status: 28160/60000 (47%) | Loss: 0.842641\n",
      "Train Epoch: 2 | Batch Status: 28800/60000 (48%) | Loss: 0.631038\n",
      "Train Epoch: 2 | Batch Status: 29440/60000 (49%) | Loss: 0.654967\n",
      "Train Epoch: 2 | Batch Status: 30080/60000 (50%) | Loss: 0.551885\n",
      "Train Epoch: 2 | Batch Status: 30720/60000 (51%) | Loss: 0.966596\n",
      "Train Epoch: 2 | Batch Status: 31360/60000 (52%) | Loss: 0.519259\n",
      "Train Epoch: 2 | Batch Status: 32000/60000 (53%) | Loss: 0.524313\n",
      "Train Epoch: 2 | Batch Status: 32640/60000 (54%) | Loss: 0.310619\n",
      "Train Epoch: 2 | Batch Status: 33280/60000 (55%) | Loss: 0.661676\n",
      "Train Epoch: 2 | Batch Status: 33920/60000 (57%) | Loss: 0.531410\n",
      "Train Epoch: 2 | Batch Status: 34560/60000 (58%) | Loss: 0.643731\n",
      "Train Epoch: 2 | Batch Status: 35200/60000 (59%) | Loss: 0.457346\n",
      "Train Epoch: 2 | Batch Status: 35840/60000 (60%) | Loss: 0.505712\n",
      "Train Epoch: 2 | Batch Status: 36480/60000 (61%) | Loss: 0.531527\n",
      "Train Epoch: 2 | Batch Status: 37120/60000 (62%) | Loss: 0.482441\n",
      "Train Epoch: 2 | Batch Status: 37760/60000 (63%) | Loss: 0.757712\n",
      "Train Epoch: 2 | Batch Status: 38400/60000 (64%) | Loss: 0.723251\n",
      "Train Epoch: 2 | Batch Status: 39040/60000 (65%) | Loss: 0.717434\n",
      "Train Epoch: 2 | Batch Status: 39680/60000 (66%) | Loss: 0.650572\n",
      "Train Epoch: 2 | Batch Status: 40320/60000 (67%) | Loss: 0.475678\n",
      "Train Epoch: 2 | Batch Status: 40960/60000 (68%) | Loss: 0.617756\n",
      "Train Epoch: 2 | Batch Status: 41600/60000 (69%) | Loss: 0.434534\n",
      "Train Epoch: 2 | Batch Status: 42240/60000 (70%) | Loss: 0.476094\n",
      "Train Epoch: 2 | Batch Status: 42880/60000 (71%) | Loss: 0.459627\n",
      "Train Epoch: 2 | Batch Status: 43520/60000 (72%) | Loss: 0.630736\n",
      "Train Epoch: 2 | Batch Status: 44160/60000 (74%) | Loss: 0.446971\n",
      "Train Epoch: 2 | Batch Status: 44800/60000 (75%) | Loss: 0.333626\n",
      "Train Epoch: 2 | Batch Status: 45440/60000 (76%) | Loss: 0.434235\n",
      "Train Epoch: 2 | Batch Status: 46080/60000 (77%) | Loss: 0.461390\n",
      "Train Epoch: 2 | Batch Status: 46720/60000 (78%) | Loss: 0.568855\n",
      "Train Epoch: 2 | Batch Status: 47360/60000 (79%) | Loss: 0.528932\n",
      "Train Epoch: 2 | Batch Status: 48000/60000 (80%) | Loss: 0.480200\n",
      "Train Epoch: 2 | Batch Status: 48640/60000 (81%) | Loss: 0.466261\n",
      "Train Epoch: 2 | Batch Status: 49280/60000 (82%) | Loss: 0.376352\n",
      "Train Epoch: 2 | Batch Status: 49920/60000 (83%) | Loss: 0.493627\n",
      "Train Epoch: 2 | Batch Status: 50560/60000 (84%) | Loss: 0.421263\n",
      "Train Epoch: 2 | Batch Status: 51200/60000 (85%) | Loss: 0.482466\n",
      "Train Epoch: 2 | Batch Status: 51840/60000 (86%) | Loss: 0.512046\n",
      "Train Epoch: 2 | Batch Status: 52480/60000 (87%) | Loss: 0.351560\n",
      "Train Epoch: 2 | Batch Status: 53120/60000 (88%) | Loss: 0.356382\n",
      "Train Epoch: 2 | Batch Status: 53760/60000 (90%) | Loss: 0.385190\n",
      "Train Epoch: 2 | Batch Status: 54400/60000 (91%) | Loss: 0.311983\n",
      "Train Epoch: 2 | Batch Status: 55040/60000 (92%) | Loss: 0.452043\n",
      "Train Epoch: 2 | Batch Status: 55680/60000 (93%) | Loss: 0.354735\n",
      "Train Epoch: 2 | Batch Status: 56320/60000 (94%) | Loss: 0.910611\n",
      "Train Epoch: 2 | Batch Status: 56960/60000 (95%) | Loss: 0.611132\n",
      "Train Epoch: 2 | Batch Status: 57600/60000 (96%) | Loss: 0.424826\n",
      "Train Epoch: 2 | Batch Status: 58240/60000 (97%) | Loss: 0.783117\n",
      "Train Epoch: 2 | Batch Status: 58880/60000 (98%) | Loss: 0.328021\n",
      "Train Epoch: 2 | Batch Status: 59520/60000 (99%) | Loss: 0.406207\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0071, Accuracy: 8626/10000 (86%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 3 | Batch Status: 0/60000 (0%) | Loss: 0.337476\n",
      "Train Epoch: 3 | Batch Status: 640/60000 (1%) | Loss: 0.511610\n",
      "Train Epoch: 3 | Batch Status: 1280/60000 (2%) | Loss: 0.385649\n",
      "Train Epoch: 3 | Batch Status: 1920/60000 (3%) | Loss: 0.361190\n",
      "Train Epoch: 3 | Batch Status: 2560/60000 (4%) | Loss: 0.605636\n",
      "Train Epoch: 3 | Batch Status: 3200/60000 (5%) | Loss: 0.515337\n",
      "Train Epoch: 3 | Batch Status: 3840/60000 (6%) | Loss: 0.403445\n",
      "Train Epoch: 3 | Batch Status: 4480/60000 (7%) | Loss: 0.395496\n",
      "Train Epoch: 3 | Batch Status: 5120/60000 (9%) | Loss: 0.422588\n",
      "Train Epoch: 3 | Batch Status: 5760/60000 (10%) | Loss: 0.340054\n",
      "Train Epoch: 3 | Batch Status: 6400/60000 (11%) | Loss: 0.429637\n",
      "Train Epoch: 3 | Batch Status: 7040/60000 (12%) | Loss: 0.260507\n",
      "Train Epoch: 3 | Batch Status: 7680/60000 (13%) | Loss: 0.334455\n",
      "Train Epoch: 3 | Batch Status: 8320/60000 (14%) | Loss: 0.412904\n",
      "Train Epoch: 3 | Batch Status: 8960/60000 (15%) | Loss: 0.752224\n",
      "Train Epoch: 3 | Batch Status: 9600/60000 (16%) | Loss: 0.439568\n",
      "Train Epoch: 3 | Batch Status: 10240/60000 (17%) | Loss: 0.250830\n",
      "Train Epoch: 3 | Batch Status: 10880/60000 (18%) | Loss: 0.625119\n",
      "Train Epoch: 3 | Batch Status: 11520/60000 (19%) | Loss: 0.425325\n",
      "Train Epoch: 3 | Batch Status: 12160/60000 (20%) | Loss: 0.391908\n",
      "Train Epoch: 3 | Batch Status: 12800/60000 (21%) | Loss: 0.342479\n",
      "Train Epoch: 3 | Batch Status: 13440/60000 (22%) | Loss: 0.316194\n",
      "Train Epoch: 3 | Batch Status: 14080/60000 (23%) | Loss: 0.282000\n",
      "Train Epoch: 3 | Batch Status: 14720/60000 (25%) | Loss: 0.196216\n",
      "Train Epoch: 3 | Batch Status: 15360/60000 (26%) | Loss: 0.302859\n",
      "Train Epoch: 3 | Batch Status: 16000/60000 (27%) | Loss: 0.528272\n",
      "Train Epoch: 3 | Batch Status: 16640/60000 (28%) | Loss: 0.351896\n",
      "Train Epoch: 3 | Batch Status: 17280/60000 (29%) | Loss: 0.329562\n",
      "Train Epoch: 3 | Batch Status: 17920/60000 (30%) | Loss: 0.376887\n",
      "Train Epoch: 3 | Batch Status: 18560/60000 (31%) | Loss: 0.311477\n",
      "Train Epoch: 3 | Batch Status: 19200/60000 (32%) | Loss: 0.304346\n",
      "Train Epoch: 3 | Batch Status: 19840/60000 (33%) | Loss: 0.380675\n",
      "Train Epoch: 3 | Batch Status: 20480/60000 (34%) | Loss: 0.214109\n",
      "Train Epoch: 3 | Batch Status: 21120/60000 (35%) | Loss: 0.240798\n",
      "Train Epoch: 3 | Batch Status: 21760/60000 (36%) | Loss: 0.266088\n",
      "Train Epoch: 3 | Batch Status: 22400/60000 (37%) | Loss: 0.353483\n",
      "Train Epoch: 3 | Batch Status: 23040/60000 (38%) | Loss: 0.427731\n",
      "Train Epoch: 3 | Batch Status: 23680/60000 (39%) | Loss: 0.350229\n",
      "Train Epoch: 3 | Batch Status: 24320/60000 (41%) | Loss: 0.304286\n",
      "Train Epoch: 3 | Batch Status: 24960/60000 (42%) | Loss: 0.418244\n",
      "Train Epoch: 3 | Batch Status: 25600/60000 (43%) | Loss: 0.467740\n",
      "Train Epoch: 3 | Batch Status: 26240/60000 (44%) | Loss: 0.401373\n",
      "Train Epoch: 3 | Batch Status: 26880/60000 (45%) | Loss: 0.380846\n",
      "Train Epoch: 3 | Batch Status: 27520/60000 (46%) | Loss: 0.297258\n",
      "Train Epoch: 3 | Batch Status: 28160/60000 (47%) | Loss: 0.318203\n",
      "Train Epoch: 3 | Batch Status: 28800/60000 (48%) | Loss: 0.191634\n",
      "Train Epoch: 3 | Batch Status: 29440/60000 (49%) | Loss: 0.385193\n",
      "Train Epoch: 3 | Batch Status: 30080/60000 (50%) | Loss: 0.390982\n",
      "Train Epoch: 3 | Batch Status: 30720/60000 (51%) | Loss: 0.540411\n",
      "Train Epoch: 3 | Batch Status: 31360/60000 (52%) | Loss: 0.410893\n",
      "Train Epoch: 3 | Batch Status: 32000/60000 (53%) | Loss: 0.317389\n",
      "Train Epoch: 3 | Batch Status: 32640/60000 (54%) | Loss: 0.296298\n",
      "Train Epoch: 3 | Batch Status: 33280/60000 (55%) | Loss: 0.388604\n",
      "Train Epoch: 3 | Batch Status: 33920/60000 (57%) | Loss: 0.283554\n",
      "Train Epoch: 3 | Batch Status: 34560/60000 (58%) | Loss: 0.503052\n",
      "Train Epoch: 3 | Batch Status: 35200/60000 (59%) | Loss: 0.319737\n",
      "Train Epoch: 3 | Batch Status: 35840/60000 (60%) | Loss: 0.362000\n",
      "Train Epoch: 3 | Batch Status: 36480/60000 (61%) | Loss: 0.268577\n",
      "Train Epoch: 3 | Batch Status: 37120/60000 (62%) | Loss: 0.303386\n",
      "Train Epoch: 3 | Batch Status: 37760/60000 (63%) | Loss: 0.375150\n",
      "Train Epoch: 3 | Batch Status: 38400/60000 (64%) | Loss: 0.252970\n",
      "Train Epoch: 3 | Batch Status: 39040/60000 (65%) | Loss: 0.387329\n",
      "Train Epoch: 3 | Batch Status: 39680/60000 (66%) | Loss: 0.583264\n",
      "Train Epoch: 3 | Batch Status: 40320/60000 (67%) | Loss: 0.197145\n",
      "Train Epoch: 3 | Batch Status: 40960/60000 (68%) | Loss: 0.252594\n",
      "Train Epoch: 3 | Batch Status: 41600/60000 (69%) | Loss: 0.267670\n",
      "Train Epoch: 3 | Batch Status: 42240/60000 (70%) | Loss: 0.297066\n",
      "Train Epoch: 3 | Batch Status: 42880/60000 (71%) | Loss: 0.319643\n",
      "Train Epoch: 3 | Batch Status: 43520/60000 (72%) | Loss: 0.192198\n",
      "Train Epoch: 3 | Batch Status: 44160/60000 (74%) | Loss: 0.231330\n",
      "Train Epoch: 3 | Batch Status: 44800/60000 (75%) | Loss: 0.284612\n",
      "Train Epoch: 3 | Batch Status: 45440/60000 (76%) | Loss: 0.313768\n",
      "Train Epoch: 3 | Batch Status: 46080/60000 (77%) | Loss: 0.328586\n",
      "Train Epoch: 3 | Batch Status: 46720/60000 (78%) | Loss: 0.215625\n",
      "Train Epoch: 3 | Batch Status: 47360/60000 (79%) | Loss: 0.357197\n",
      "Train Epoch: 3 | Batch Status: 48000/60000 (80%) | Loss: 0.339102\n",
      "Train Epoch: 3 | Batch Status: 48640/60000 (81%) | Loss: 0.494784\n",
      "Train Epoch: 3 | Batch Status: 49280/60000 (82%) | Loss: 0.283571\n",
      "Train Epoch: 3 | Batch Status: 49920/60000 (83%) | Loss: 0.429992\n",
      "Train Epoch: 3 | Batch Status: 50560/60000 (84%) | Loss: 0.534512\n",
      "Train Epoch: 3 | Batch Status: 51200/60000 (85%) | Loss: 0.154713\n",
      "Train Epoch: 3 | Batch Status: 51840/60000 (86%) | Loss: 0.225554\n",
      "Train Epoch: 3 | Batch Status: 52480/60000 (87%) | Loss: 0.211285\n",
      "Train Epoch: 3 | Batch Status: 53120/60000 (88%) | Loss: 0.137460\n",
      "Train Epoch: 3 | Batch Status: 53760/60000 (90%) | Loss: 0.395375\n",
      "Train Epoch: 3 | Batch Status: 54400/60000 (91%) | Loss: 0.256753\n",
      "Train Epoch: 3 | Batch Status: 55040/60000 (92%) | Loss: 0.288888\n",
      "Train Epoch: 3 | Batch Status: 55680/60000 (93%) | Loss: 0.173437\n",
      "Train Epoch: 3 | Batch Status: 56320/60000 (94%) | Loss: 0.531661\n",
      "Train Epoch: 3 | Batch Status: 56960/60000 (95%) | Loss: 0.242174\n",
      "Train Epoch: 3 | Batch Status: 57600/60000 (96%) | Loss: 0.269265\n",
      "Train Epoch: 3 | Batch Status: 58240/60000 (97%) | Loss: 0.275610\n",
      "Train Epoch: 3 | Batch Status: 58880/60000 (98%) | Loss: 0.254115\n",
      "Train Epoch: 3 | Batch Status: 59520/60000 (99%) | Loss: 0.310852\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0043, Accuracy: 9158/10000 (92%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 4 | Batch Status: 0/60000 (0%) | Loss: 0.220515\n",
      "Train Epoch: 4 | Batch Status: 640/60000 (1%) | Loss: 0.352305\n",
      "Train Epoch: 4 | Batch Status: 1280/60000 (2%) | Loss: 0.358852\n",
      "Train Epoch: 4 | Batch Status: 1920/60000 (3%) | Loss: 0.154061\n",
      "Train Epoch: 4 | Batch Status: 2560/60000 (4%) | Loss: 0.328003\n",
      "Train Epoch: 4 | Batch Status: 3200/60000 (5%) | Loss: 0.400771\n",
      "Train Epoch: 4 | Batch Status: 3840/60000 (6%) | Loss: 0.260816\n",
      "Train Epoch: 4 | Batch Status: 4480/60000 (7%) | Loss: 0.246700\n",
      "Train Epoch: 4 | Batch Status: 5120/60000 (9%) | Loss: 0.298242\n",
      "Train Epoch: 4 | Batch Status: 5760/60000 (10%) | Loss: 0.282438\n",
      "Train Epoch: 4 | Batch Status: 6400/60000 (11%) | Loss: 0.126570\n",
      "Train Epoch: 4 | Batch Status: 7040/60000 (12%) | Loss: 0.215757\n",
      "Train Epoch: 4 | Batch Status: 7680/60000 (13%) | Loss: 0.226975\n",
      "Train Epoch: 4 | Batch Status: 8320/60000 (14%) | Loss: 0.176584\n",
      "Train Epoch: 4 | Batch Status: 8960/60000 (15%) | Loss: 0.394923\n",
      "Train Epoch: 4 | Batch Status: 9600/60000 (16%) | Loss: 0.281194\n",
      "Train Epoch: 4 | Batch Status: 10240/60000 (17%) | Loss: 0.421393\n",
      "Train Epoch: 4 | Batch Status: 10880/60000 (18%) | Loss: 0.308331\n",
      "Train Epoch: 4 | Batch Status: 11520/60000 (19%) | Loss: 0.343327\n",
      "Train Epoch: 4 | Batch Status: 12160/60000 (20%) | Loss: 0.696703\n",
      "Train Epoch: 4 | Batch Status: 12800/60000 (21%) | Loss: 0.223909\n",
      "Train Epoch: 4 | Batch Status: 13440/60000 (22%) | Loss: 0.474931\n",
      "Train Epoch: 4 | Batch Status: 14080/60000 (23%) | Loss: 0.383191\n",
      "Train Epoch: 4 | Batch Status: 14720/60000 (25%) | Loss: 0.286751\n",
      "Train Epoch: 4 | Batch Status: 15360/60000 (26%) | Loss: 0.188220\n",
      "Train Epoch: 4 | Batch Status: 16000/60000 (27%) | Loss: 0.309671\n",
      "Train Epoch: 4 | Batch Status: 16640/60000 (28%) | Loss: 0.432817\n",
      "Train Epoch: 4 | Batch Status: 17280/60000 (29%) | Loss: 0.239818\n",
      "Train Epoch: 4 | Batch Status: 17920/60000 (30%) | Loss: 0.216131\n",
      "Train Epoch: 4 | Batch Status: 18560/60000 (31%) | Loss: 0.130818\n",
      "Train Epoch: 4 | Batch Status: 19200/60000 (32%) | Loss: 0.266375\n",
      "Train Epoch: 4 | Batch Status: 19840/60000 (33%) | Loss: 0.149926\n",
      "Train Epoch: 4 | Batch Status: 20480/60000 (34%) | Loss: 0.395706\n",
      "Train Epoch: 4 | Batch Status: 21120/60000 (35%) | Loss: 0.315847\n",
      "Train Epoch: 4 | Batch Status: 21760/60000 (36%) | Loss: 0.137694\n",
      "Train Epoch: 4 | Batch Status: 22400/60000 (37%) | Loss: 0.243413\n",
      "Train Epoch: 4 | Batch Status: 23040/60000 (38%) | Loss: 0.525260\n",
      "Train Epoch: 4 | Batch Status: 23680/60000 (39%) | Loss: 0.116765\n",
      "Train Epoch: 4 | Batch Status: 24320/60000 (41%) | Loss: 0.190819\n",
      "Train Epoch: 4 | Batch Status: 24960/60000 (42%) | Loss: 0.206995\n",
      "Train Epoch: 4 | Batch Status: 25600/60000 (43%) | Loss: 0.508518\n",
      "Train Epoch: 4 | Batch Status: 26240/60000 (44%) | Loss: 0.298563\n",
      "Train Epoch: 4 | Batch Status: 26880/60000 (45%) | Loss: 0.127407\n",
      "Train Epoch: 4 | Batch Status: 27520/60000 (46%) | Loss: 0.264806\n",
      "Train Epoch: 4 | Batch Status: 28160/60000 (47%) | Loss: 0.157064\n",
      "Train Epoch: 4 | Batch Status: 28800/60000 (48%) | Loss: 0.241123\n",
      "Train Epoch: 4 | Batch Status: 29440/60000 (49%) | Loss: 0.228894\n",
      "Train Epoch: 4 | Batch Status: 30080/60000 (50%) | Loss: 0.445783\n",
      "Train Epoch: 4 | Batch Status: 30720/60000 (51%) | Loss: 0.285086\n",
      "Train Epoch: 4 | Batch Status: 31360/60000 (52%) | Loss: 0.391241\n",
      "Train Epoch: 4 | Batch Status: 32000/60000 (53%) | Loss: 0.097572\n",
      "Train Epoch: 4 | Batch Status: 32640/60000 (54%) | Loss: 0.259352\n",
      "Train Epoch: 4 | Batch Status: 33280/60000 (55%) | Loss: 0.174476\n",
      "Train Epoch: 4 | Batch Status: 33920/60000 (57%) | Loss: 0.224210\n",
      "Train Epoch: 4 | Batch Status: 34560/60000 (58%) | Loss: 0.226597\n",
      "Train Epoch: 4 | Batch Status: 35200/60000 (59%) | Loss: 0.553319\n",
      "Train Epoch: 4 | Batch Status: 35840/60000 (60%) | Loss: 0.352842\n",
      "Train Epoch: 4 | Batch Status: 36480/60000 (61%) | Loss: 0.140115\n",
      "Train Epoch: 4 | Batch Status: 37120/60000 (62%) | Loss: 0.173567\n",
      "Train Epoch: 4 | Batch Status: 37760/60000 (63%) | Loss: 0.132405\n",
      "Train Epoch: 4 | Batch Status: 38400/60000 (64%) | Loss: 0.140007\n",
      "Train Epoch: 4 | Batch Status: 39040/60000 (65%) | Loss: 0.261369\n",
      "Train Epoch: 4 | Batch Status: 39680/60000 (66%) | Loss: 0.227024\n",
      "Train Epoch: 4 | Batch Status: 40320/60000 (67%) | Loss: 0.152354\n",
      "Train Epoch: 4 | Batch Status: 40960/60000 (68%) | Loss: 0.266700\n",
      "Train Epoch: 4 | Batch Status: 41600/60000 (69%) | Loss: 0.127466\n",
      "Train Epoch: 4 | Batch Status: 42240/60000 (70%) | Loss: 0.301648\n",
      "Train Epoch: 4 | Batch Status: 42880/60000 (71%) | Loss: 0.190167\n",
      "Train Epoch: 4 | Batch Status: 43520/60000 (72%) | Loss: 0.371177\n",
      "Train Epoch: 4 | Batch Status: 44160/60000 (74%) | Loss: 0.261626\n",
      "Train Epoch: 4 | Batch Status: 44800/60000 (75%) | Loss: 0.199688\n",
      "Train Epoch: 4 | Batch Status: 45440/60000 (76%) | Loss: 0.264095\n",
      "Train Epoch: 4 | Batch Status: 46080/60000 (77%) | Loss: 0.155491\n",
      "Train Epoch: 4 | Batch Status: 46720/60000 (78%) | Loss: 0.165017\n",
      "Train Epoch: 4 | Batch Status: 47360/60000 (79%) | Loss: 0.260953\n",
      "Train Epoch: 4 | Batch Status: 48000/60000 (80%) | Loss: 0.407149\n",
      "Train Epoch: 4 | Batch Status: 48640/60000 (81%) | Loss: 0.347141\n",
      "Train Epoch: 4 | Batch Status: 49280/60000 (82%) | Loss: 0.275356\n",
      "Train Epoch: 4 | Batch Status: 49920/60000 (83%) | Loss: 0.177885\n",
      "Train Epoch: 4 | Batch Status: 50560/60000 (84%) | Loss: 0.234620\n",
      "Train Epoch: 4 | Batch Status: 51200/60000 (85%) | Loss: 0.356796\n",
      "Train Epoch: 4 | Batch Status: 51840/60000 (86%) | Loss: 0.325896\n",
      "Train Epoch: 4 | Batch Status: 52480/60000 (87%) | Loss: 0.230148\n",
      "Train Epoch: 4 | Batch Status: 53120/60000 (88%) | Loss: 0.229214\n",
      "Train Epoch: 4 | Batch Status: 53760/60000 (90%) | Loss: 0.251594\n",
      "Train Epoch: 4 | Batch Status: 54400/60000 (91%) | Loss: 0.220785\n",
      "Train Epoch: 4 | Batch Status: 55040/60000 (92%) | Loss: 0.225068\n",
      "Train Epoch: 4 | Batch Status: 55680/60000 (93%) | Loss: 0.169243\n",
      "Train Epoch: 4 | Batch Status: 56320/60000 (94%) | Loss: 0.200550\n",
      "Train Epoch: 4 | Batch Status: 56960/60000 (95%) | Loss: 0.199090\n",
      "Train Epoch: 4 | Batch Status: 57600/60000 (96%) | Loss: 0.406323\n",
      "Train Epoch: 4 | Batch Status: 58240/60000 (97%) | Loss: 0.308143\n",
      "Train Epoch: 4 | Batch Status: 58880/60000 (98%) | Loss: 0.114434\n",
      "Train Epoch: 4 | Batch Status: 59520/60000 (99%) | Loss: 0.254054\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0033, Accuracy: 9388/10000 (94%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 5 | Batch Status: 0/60000 (0%) | Loss: 0.139441\n",
      "Train Epoch: 5 | Batch Status: 640/60000 (1%) | Loss: 0.322065\n",
      "Train Epoch: 5 | Batch Status: 1280/60000 (2%) | Loss: 0.205902\n",
      "Train Epoch: 5 | Batch Status: 1920/60000 (3%) | Loss: 0.115826\n",
      "Train Epoch: 5 | Batch Status: 2560/60000 (4%) | Loss: 0.459465\n",
      "Train Epoch: 5 | Batch Status: 3200/60000 (5%) | Loss: 0.151750\n",
      "Train Epoch: 5 | Batch Status: 3840/60000 (6%) | Loss: 0.210587\n",
      "Train Epoch: 5 | Batch Status: 4480/60000 (7%) | Loss: 0.184848\n",
      "Train Epoch: 5 | Batch Status: 5120/60000 (9%) | Loss: 0.150737\n",
      "Train Epoch: 5 | Batch Status: 5760/60000 (10%) | Loss: 0.286069\n",
      "Train Epoch: 5 | Batch Status: 6400/60000 (11%) | Loss: 0.140497\n",
      "Train Epoch: 5 | Batch Status: 7040/60000 (12%) | Loss: 0.112173\n",
      "Train Epoch: 5 | Batch Status: 7680/60000 (13%) | Loss: 0.152365\n",
      "Train Epoch: 5 | Batch Status: 8320/60000 (14%) | Loss: 0.182370\n",
      "Train Epoch: 5 | Batch Status: 8960/60000 (15%) | Loss: 0.196601\n",
      "Train Epoch: 5 | Batch Status: 9600/60000 (16%) | Loss: 0.210015\n",
      "Train Epoch: 5 | Batch Status: 10240/60000 (17%) | Loss: 0.133577\n",
      "Train Epoch: 5 | Batch Status: 10880/60000 (18%) | Loss: 0.184377\n",
      "Train Epoch: 5 | Batch Status: 11520/60000 (19%) | Loss: 0.200683\n",
      "Train Epoch: 5 | Batch Status: 12160/60000 (20%) | Loss: 0.142657\n",
      "Train Epoch: 5 | Batch Status: 12800/60000 (21%) | Loss: 0.116222\n",
      "Train Epoch: 5 | Batch Status: 13440/60000 (22%) | Loss: 0.228612\n",
      "Train Epoch: 5 | Batch Status: 14080/60000 (23%) | Loss: 0.370375\n",
      "Train Epoch: 5 | Batch Status: 14720/60000 (25%) | Loss: 0.134575\n",
      "Train Epoch: 5 | Batch Status: 15360/60000 (26%) | Loss: 0.095308\n",
      "Train Epoch: 5 | Batch Status: 16000/60000 (27%) | Loss: 0.206850\n",
      "Train Epoch: 5 | Batch Status: 16640/60000 (28%) | Loss: 0.107443\n",
      "Train Epoch: 5 | Batch Status: 17280/60000 (29%) | Loss: 0.306845\n",
      "Train Epoch: 5 | Batch Status: 17920/60000 (30%) | Loss: 0.130519\n",
      "Train Epoch: 5 | Batch Status: 18560/60000 (31%) | Loss: 0.180904\n",
      "Train Epoch: 5 | Batch Status: 19200/60000 (32%) | Loss: 0.168718\n",
      "Train Epoch: 5 | Batch Status: 19840/60000 (33%) | Loss: 0.175039\n",
      "Train Epoch: 5 | Batch Status: 20480/60000 (34%) | Loss: 0.167878\n",
      "Train Epoch: 5 | Batch Status: 21120/60000 (35%) | Loss: 0.138680\n",
      "Train Epoch: 5 | Batch Status: 21760/60000 (36%) | Loss: 0.075898\n",
      "Train Epoch: 5 | Batch Status: 22400/60000 (37%) | Loss: 0.082628\n",
      "Train Epoch: 5 | Batch Status: 23040/60000 (38%) | Loss: 0.099492\n",
      "Train Epoch: 5 | Batch Status: 23680/60000 (39%) | Loss: 0.220303\n",
      "Train Epoch: 5 | Batch Status: 24320/60000 (41%) | Loss: 0.257767\n",
      "Train Epoch: 5 | Batch Status: 24960/60000 (42%) | Loss: 0.130185\n",
      "Train Epoch: 5 | Batch Status: 25600/60000 (43%) | Loss: 0.148478\n",
      "Train Epoch: 5 | Batch Status: 26240/60000 (44%) | Loss: 0.241072\n",
      "Train Epoch: 5 | Batch Status: 26880/60000 (45%) | Loss: 0.221621\n",
      "Train Epoch: 5 | Batch Status: 27520/60000 (46%) | Loss: 0.366916\n",
      "Train Epoch: 5 | Batch Status: 28160/60000 (47%) | Loss: 0.109971\n",
      "Train Epoch: 5 | Batch Status: 28800/60000 (48%) | Loss: 0.224405\n",
      "Train Epoch: 5 | Batch Status: 29440/60000 (49%) | Loss: 0.232906\n",
      "Train Epoch: 5 | Batch Status: 30080/60000 (50%) | Loss: 0.140706\n",
      "Train Epoch: 5 | Batch Status: 30720/60000 (51%) | Loss: 0.207351\n",
      "Train Epoch: 5 | Batch Status: 31360/60000 (52%) | Loss: 0.216320\n",
      "Train Epoch: 5 | Batch Status: 32000/60000 (53%) | Loss: 0.127928\n",
      "Train Epoch: 5 | Batch Status: 32640/60000 (54%) | Loss: 0.210544\n",
      "Train Epoch: 5 | Batch Status: 33280/60000 (55%) | Loss: 0.215873\n",
      "Train Epoch: 5 | Batch Status: 33920/60000 (57%) | Loss: 0.152607\n",
      "Train Epoch: 5 | Batch Status: 34560/60000 (58%) | Loss: 0.176057\n",
      "Train Epoch: 5 | Batch Status: 35200/60000 (59%) | Loss: 0.144308\n",
      "Train Epoch: 5 | Batch Status: 35840/60000 (60%) | Loss: 0.202145\n",
      "Train Epoch: 5 | Batch Status: 36480/60000 (61%) | Loss: 0.144259\n",
      "Train Epoch: 5 | Batch Status: 37120/60000 (62%) | Loss: 0.167561\n",
      "Train Epoch: 5 | Batch Status: 37760/60000 (63%) | Loss: 0.251518\n",
      "Train Epoch: 5 | Batch Status: 38400/60000 (64%) | Loss: 0.175412\n",
      "Train Epoch: 5 | Batch Status: 39040/60000 (65%) | Loss: 0.132845\n",
      "Train Epoch: 5 | Batch Status: 39680/60000 (66%) | Loss: 0.149881\n",
      "Train Epoch: 5 | Batch Status: 40320/60000 (67%) | Loss: 0.129769\n",
      "Train Epoch: 5 | Batch Status: 40960/60000 (68%) | Loss: 0.102164\n",
      "Train Epoch: 5 | Batch Status: 41600/60000 (69%) | Loss: 0.169931\n",
      "Train Epoch: 5 | Batch Status: 42240/60000 (70%) | Loss: 0.369557\n",
      "Train Epoch: 5 | Batch Status: 42880/60000 (71%) | Loss: 0.111195\n",
      "Train Epoch: 5 | Batch Status: 43520/60000 (72%) | Loss: 0.236452\n",
      "Train Epoch: 5 | Batch Status: 44160/60000 (74%) | Loss: 0.202682\n",
      "Train Epoch: 5 | Batch Status: 44800/60000 (75%) | Loss: 0.125303\n",
      "Train Epoch: 5 | Batch Status: 45440/60000 (76%) | Loss: 0.185482\n",
      "Train Epoch: 5 | Batch Status: 46080/60000 (77%) | Loss: 0.115060\n",
      "Train Epoch: 5 | Batch Status: 46720/60000 (78%) | Loss: 0.163113\n",
      "Train Epoch: 5 | Batch Status: 47360/60000 (79%) | Loss: 0.318928\n",
      "Train Epoch: 5 | Batch Status: 48000/60000 (80%) | Loss: 0.115724\n",
      "Train Epoch: 5 | Batch Status: 48640/60000 (81%) | Loss: 0.065327\n",
      "Train Epoch: 5 | Batch Status: 49280/60000 (82%) | Loss: 0.207345\n",
      "Train Epoch: 5 | Batch Status: 49920/60000 (83%) | Loss: 0.127730\n",
      "Train Epoch: 5 | Batch Status: 50560/60000 (84%) | Loss: 0.120114\n",
      "Train Epoch: 5 | Batch Status: 51200/60000 (85%) | Loss: 0.466700\n",
      "Train Epoch: 5 | Batch Status: 51840/60000 (86%) | Loss: 0.110961\n",
      "Train Epoch: 5 | Batch Status: 52480/60000 (87%) | Loss: 0.096607\n",
      "Train Epoch: 5 | Batch Status: 53120/60000 (88%) | Loss: 0.235521\n",
      "Train Epoch: 5 | Batch Status: 53760/60000 (90%) | Loss: 0.129751\n",
      "Train Epoch: 5 | Batch Status: 54400/60000 (91%) | Loss: 0.241607\n",
      "Train Epoch: 5 | Batch Status: 55040/60000 (92%) | Loss: 0.256490\n",
      "Train Epoch: 5 | Batch Status: 55680/60000 (93%) | Loss: 0.106053\n",
      "Train Epoch: 5 | Batch Status: 56320/60000 (94%) | Loss: 0.127847\n",
      "Train Epoch: 5 | Batch Status: 56960/60000 (95%) | Loss: 0.095002\n",
      "Train Epoch: 5 | Batch Status: 57600/60000 (96%) | Loss: 0.158806\n",
      "Train Epoch: 5 | Batch Status: 58240/60000 (97%) | Loss: 0.067142\n",
      "Train Epoch: 5 | Batch Status: 58880/60000 (98%) | Loss: 0.232557\n",
      "Train Epoch: 5 | Batch Status: 59520/60000 (99%) | Loss: 0.207417\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0026, Accuracy: 9507/10000 (95%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 6 | Batch Status: 0/60000 (0%) | Loss: 0.167272\n",
      "Train Epoch: 6 | Batch Status: 640/60000 (1%) | Loss: 0.113330\n",
      "Train Epoch: 6 | Batch Status: 1280/60000 (2%) | Loss: 0.158945\n",
      "Train Epoch: 6 | Batch Status: 1920/60000 (3%) | Loss: 0.110604\n",
      "Train Epoch: 6 | Batch Status: 2560/60000 (4%) | Loss: 0.177959\n",
      "Train Epoch: 6 | Batch Status: 3200/60000 (5%) | Loss: 0.117874\n",
      "Train Epoch: 6 | Batch Status: 3840/60000 (6%) | Loss: 0.196022\n",
      "Train Epoch: 6 | Batch Status: 4480/60000 (7%) | Loss: 0.147656\n",
      "Train Epoch: 6 | Batch Status: 5120/60000 (9%) | Loss: 0.093476\n",
      "Train Epoch: 6 | Batch Status: 5760/60000 (10%) | Loss: 0.177635\n",
      "Train Epoch: 6 | Batch Status: 6400/60000 (11%) | Loss: 0.070759\n",
      "Train Epoch: 6 | Batch Status: 7040/60000 (12%) | Loss: 0.087208\n",
      "Train Epoch: 6 | Batch Status: 7680/60000 (13%) | Loss: 0.135003\n",
      "Train Epoch: 6 | Batch Status: 8320/60000 (14%) | Loss: 0.166378\n",
      "Train Epoch: 6 | Batch Status: 8960/60000 (15%) | Loss: 0.113939\n",
      "Train Epoch: 6 | Batch Status: 9600/60000 (16%) | Loss: 0.110859\n",
      "Train Epoch: 6 | Batch Status: 10240/60000 (17%) | Loss: 0.198360\n",
      "Train Epoch: 6 | Batch Status: 10880/60000 (18%) | Loss: 0.164668\n",
      "Train Epoch: 6 | Batch Status: 11520/60000 (19%) | Loss: 0.084069\n",
      "Train Epoch: 6 | Batch Status: 12160/60000 (20%) | Loss: 0.235359\n",
      "Train Epoch: 6 | Batch Status: 12800/60000 (21%) | Loss: 0.219183\n",
      "Train Epoch: 6 | Batch Status: 13440/60000 (22%) | Loss: 0.186774\n",
      "Train Epoch: 6 | Batch Status: 14080/60000 (23%) | Loss: 0.126400\n",
      "Train Epoch: 6 | Batch Status: 14720/60000 (25%) | Loss: 0.173296\n",
      "Train Epoch: 6 | Batch Status: 15360/60000 (26%) | Loss: 0.038122\n",
      "Train Epoch: 6 | Batch Status: 16000/60000 (27%) | Loss: 0.140182\n",
      "Train Epoch: 6 | Batch Status: 16640/60000 (28%) | Loss: 0.216528\n",
      "Train Epoch: 6 | Batch Status: 17280/60000 (29%) | Loss: 0.249746\n",
      "Train Epoch: 6 | Batch Status: 17920/60000 (30%) | Loss: 0.086651\n",
      "Train Epoch: 6 | Batch Status: 18560/60000 (31%) | Loss: 0.076094\n",
      "Train Epoch: 6 | Batch Status: 19200/60000 (32%) | Loss: 0.172244\n",
      "Train Epoch: 6 | Batch Status: 19840/60000 (33%) | Loss: 0.120839\n",
      "Train Epoch: 6 | Batch Status: 20480/60000 (34%) | Loss: 0.132076\n",
      "Train Epoch: 6 | Batch Status: 21120/60000 (35%) | Loss: 0.123983\n",
      "Train Epoch: 6 | Batch Status: 21760/60000 (36%) | Loss: 0.187828\n",
      "Train Epoch: 6 | Batch Status: 22400/60000 (37%) | Loss: 0.137021\n",
      "Train Epoch: 6 | Batch Status: 23040/60000 (38%) | Loss: 0.118112\n",
      "Train Epoch: 6 | Batch Status: 23680/60000 (39%) | Loss: 0.123740\n",
      "Train Epoch: 6 | Batch Status: 24320/60000 (41%) | Loss: 0.223364\n",
      "Train Epoch: 6 | Batch Status: 24960/60000 (42%) | Loss: 0.141415\n",
      "Train Epoch: 6 | Batch Status: 25600/60000 (43%) | Loss: 0.273103\n",
      "Train Epoch: 6 | Batch Status: 26240/60000 (44%) | Loss: 0.150763\n",
      "Train Epoch: 6 | Batch Status: 26880/60000 (45%) | Loss: 0.175914\n",
      "Train Epoch: 6 | Batch Status: 27520/60000 (46%) | Loss: 0.159277\n",
      "Train Epoch: 6 | Batch Status: 28160/60000 (47%) | Loss: 0.236450\n",
      "Train Epoch: 6 | Batch Status: 28800/60000 (48%) | Loss: 0.088482\n",
      "Train Epoch: 6 | Batch Status: 29440/60000 (49%) | Loss: 0.174172\n",
      "Train Epoch: 6 | Batch Status: 30080/60000 (50%) | Loss: 0.204467\n",
      "Train Epoch: 6 | Batch Status: 30720/60000 (51%) | Loss: 0.086759\n",
      "Train Epoch: 6 | Batch Status: 31360/60000 (52%) | Loss: 0.305105\n",
      "Train Epoch: 6 | Batch Status: 32000/60000 (53%) | Loss: 0.067866\n",
      "Train Epoch: 6 | Batch Status: 32640/60000 (54%) | Loss: 0.041347\n",
      "Train Epoch: 6 | Batch Status: 33280/60000 (55%) | Loss: 0.132197\n",
      "Train Epoch: 6 | Batch Status: 33920/60000 (57%) | Loss: 0.115630\n",
      "Train Epoch: 6 | Batch Status: 34560/60000 (58%) | Loss: 0.085314\n",
      "Train Epoch: 6 | Batch Status: 35200/60000 (59%) | Loss: 0.137331\n",
      "Train Epoch: 6 | Batch Status: 35840/60000 (60%) | Loss: 0.181832\n",
      "Train Epoch: 6 | Batch Status: 36480/60000 (61%) | Loss: 0.177327\n",
      "Train Epoch: 6 | Batch Status: 37120/60000 (62%) | Loss: 0.221169\n",
      "Train Epoch: 6 | Batch Status: 37760/60000 (63%) | Loss: 0.199279\n",
      "Train Epoch: 6 | Batch Status: 38400/60000 (64%) | Loss: 0.060449\n",
      "Train Epoch: 6 | Batch Status: 39040/60000 (65%) | Loss: 0.052405\n",
      "Train Epoch: 6 | Batch Status: 39680/60000 (66%) | Loss: 0.126292\n",
      "Train Epoch: 6 | Batch Status: 40320/60000 (67%) | Loss: 0.299095\n",
      "Train Epoch: 6 | Batch Status: 40960/60000 (68%) | Loss: 0.233213\n",
      "Train Epoch: 6 | Batch Status: 41600/60000 (69%) | Loss: 0.128711\n",
      "Train Epoch: 6 | Batch Status: 42240/60000 (70%) | Loss: 0.074921\n",
      "Train Epoch: 6 | Batch Status: 42880/60000 (71%) | Loss: 0.157106\n",
      "Train Epoch: 6 | Batch Status: 43520/60000 (72%) | Loss: 0.113927\n",
      "Train Epoch: 6 | Batch Status: 44160/60000 (74%) | Loss: 0.218483\n",
      "Train Epoch: 6 | Batch Status: 44800/60000 (75%) | Loss: 0.088825\n",
      "Train Epoch: 6 | Batch Status: 45440/60000 (76%) | Loss: 0.176434\n",
      "Train Epoch: 6 | Batch Status: 46080/60000 (77%) | Loss: 0.119477\n",
      "Train Epoch: 6 | Batch Status: 46720/60000 (78%) | Loss: 0.191354\n",
      "Train Epoch: 6 | Batch Status: 47360/60000 (79%) | Loss: 0.070029\n",
      "Train Epoch: 6 | Batch Status: 48000/60000 (80%) | Loss: 0.144231\n",
      "Train Epoch: 6 | Batch Status: 48640/60000 (81%) | Loss: 0.138940\n",
      "Train Epoch: 6 | Batch Status: 49280/60000 (82%) | Loss: 0.080080\n",
      "Train Epoch: 6 | Batch Status: 49920/60000 (83%) | Loss: 0.104358\n",
      "Train Epoch: 6 | Batch Status: 50560/60000 (84%) | Loss: 0.178675\n",
      "Train Epoch: 6 | Batch Status: 51200/60000 (85%) | Loss: 0.090211\n",
      "Train Epoch: 6 | Batch Status: 51840/60000 (86%) | Loss: 0.183278\n",
      "Train Epoch: 6 | Batch Status: 52480/60000 (87%) | Loss: 0.120477\n",
      "Train Epoch: 6 | Batch Status: 53120/60000 (88%) | Loss: 0.103647\n",
      "Train Epoch: 6 | Batch Status: 53760/60000 (90%) | Loss: 0.089031\n",
      "Train Epoch: 6 | Batch Status: 54400/60000 (91%) | Loss: 0.163193\n",
      "Train Epoch: 6 | Batch Status: 55040/60000 (92%) | Loss: 0.105131\n",
      "Train Epoch: 6 | Batch Status: 55680/60000 (93%) | Loss: 0.086437\n",
      "Train Epoch: 6 | Batch Status: 56320/60000 (94%) | Loss: 0.112577\n",
      "Train Epoch: 6 | Batch Status: 56960/60000 (95%) | Loss: 0.203044\n",
      "Train Epoch: 6 | Batch Status: 57600/60000 (96%) | Loss: 0.157391\n",
      "Train Epoch: 6 | Batch Status: 58240/60000 (97%) | Loss: 0.066671\n",
      "Train Epoch: 6 | Batch Status: 58880/60000 (98%) | Loss: 0.110246\n",
      "Train Epoch: 6 | Batch Status: 59520/60000 (99%) | Loss: 0.217166\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0023, Accuracy: 9564/10000 (96%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 7 | Batch Status: 0/60000 (0%) | Loss: 0.127498\n",
      "Train Epoch: 7 | Batch Status: 640/60000 (1%) | Loss: 0.141052\n",
      "Train Epoch: 7 | Batch Status: 1280/60000 (2%) | Loss: 0.198317\n",
      "Train Epoch: 7 | Batch Status: 1920/60000 (3%) | Loss: 0.037940\n",
      "Train Epoch: 7 | Batch Status: 2560/60000 (4%) | Loss: 0.159154\n",
      "Train Epoch: 7 | Batch Status: 3200/60000 (5%) | Loss: 0.121375\n",
      "Train Epoch: 7 | Batch Status: 3840/60000 (6%) | Loss: 0.158458\n",
      "Train Epoch: 7 | Batch Status: 4480/60000 (7%) | Loss: 0.128519\n",
      "Train Epoch: 7 | Batch Status: 5120/60000 (9%) | Loss: 0.095050\n",
      "Train Epoch: 7 | Batch Status: 5760/60000 (10%) | Loss: 0.032312\n",
      "Train Epoch: 7 | Batch Status: 6400/60000 (11%) | Loss: 0.163237\n",
      "Train Epoch: 7 | Batch Status: 7040/60000 (12%) | Loss: 0.057845\n",
      "Train Epoch: 7 | Batch Status: 7680/60000 (13%) | Loss: 0.172205\n",
      "Train Epoch: 7 | Batch Status: 8320/60000 (14%) | Loss: 0.130559\n",
      "Train Epoch: 7 | Batch Status: 8960/60000 (15%) | Loss: 0.280056\n",
      "Train Epoch: 7 | Batch Status: 9600/60000 (16%) | Loss: 0.039232\n",
      "Train Epoch: 7 | Batch Status: 10240/60000 (17%) | Loss: 0.116250\n",
      "Train Epoch: 7 | Batch Status: 10880/60000 (18%) | Loss: 0.186257\n",
      "Train Epoch: 7 | Batch Status: 11520/60000 (19%) | Loss: 0.053050\n",
      "Train Epoch: 7 | Batch Status: 12160/60000 (20%) | Loss: 0.073259\n",
      "Train Epoch: 7 | Batch Status: 12800/60000 (21%) | Loss: 0.161083\n",
      "Train Epoch: 7 | Batch Status: 13440/60000 (22%) | Loss: 0.120409\n",
      "Train Epoch: 7 | Batch Status: 14080/60000 (23%) | Loss: 0.127418\n",
      "Train Epoch: 7 | Batch Status: 14720/60000 (25%) | Loss: 0.017460\n",
      "Train Epoch: 7 | Batch Status: 15360/60000 (26%) | Loss: 0.063886\n",
      "Train Epoch: 7 | Batch Status: 16000/60000 (27%) | Loss: 0.049329\n",
      "Train Epoch: 7 | Batch Status: 16640/60000 (28%) | Loss: 0.043667\n",
      "Train Epoch: 7 | Batch Status: 17280/60000 (29%) | Loss: 0.072033\n",
      "Train Epoch: 7 | Batch Status: 17920/60000 (30%) | Loss: 0.322669\n",
      "Train Epoch: 7 | Batch Status: 18560/60000 (31%) | Loss: 0.175503\n",
      "Train Epoch: 7 | Batch Status: 19200/60000 (32%) | Loss: 0.041996\n",
      "Train Epoch: 7 | Batch Status: 19840/60000 (33%) | Loss: 0.254364\n",
      "Train Epoch: 7 | Batch Status: 20480/60000 (34%) | Loss: 0.092743\n",
      "Train Epoch: 7 | Batch Status: 21120/60000 (35%) | Loss: 0.158956\n",
      "Train Epoch: 7 | Batch Status: 21760/60000 (36%) | Loss: 0.106676\n",
      "Train Epoch: 7 | Batch Status: 22400/60000 (37%) | Loss: 0.035133\n",
      "Train Epoch: 7 | Batch Status: 23040/60000 (38%) | Loss: 0.138942\n",
      "Train Epoch: 7 | Batch Status: 23680/60000 (39%) | Loss: 0.182234\n",
      "Train Epoch: 7 | Batch Status: 24320/60000 (41%) | Loss: 0.161577\n",
      "Train Epoch: 7 | Batch Status: 24960/60000 (42%) | Loss: 0.045984\n",
      "Train Epoch: 7 | Batch Status: 25600/60000 (43%) | Loss: 0.156151\n",
      "Train Epoch: 7 | Batch Status: 26240/60000 (44%) | Loss: 0.059112\n",
      "Train Epoch: 7 | Batch Status: 26880/60000 (45%) | Loss: 0.073889\n",
      "Train Epoch: 7 | Batch Status: 27520/60000 (46%) | Loss: 0.066875\n",
      "Train Epoch: 7 | Batch Status: 28160/60000 (47%) | Loss: 0.021806\n",
      "Train Epoch: 7 | Batch Status: 28800/60000 (48%) | Loss: 0.031562\n",
      "Train Epoch: 7 | Batch Status: 29440/60000 (49%) | Loss: 0.229929\n",
      "Train Epoch: 7 | Batch Status: 30080/60000 (50%) | Loss: 0.040312\n",
      "Train Epoch: 7 | Batch Status: 30720/60000 (51%) | Loss: 0.102498\n",
      "Train Epoch: 7 | Batch Status: 31360/60000 (52%) | Loss: 0.292197\n",
      "Train Epoch: 7 | Batch Status: 32000/60000 (53%) | Loss: 0.062207\n",
      "Train Epoch: 7 | Batch Status: 32640/60000 (54%) | Loss: 0.094351\n",
      "Train Epoch: 7 | Batch Status: 33280/60000 (55%) | Loss: 0.084762\n",
      "Train Epoch: 7 | Batch Status: 33920/60000 (57%) | Loss: 0.049941\n",
      "Train Epoch: 7 | Batch Status: 34560/60000 (58%) | Loss: 0.104650\n",
      "Train Epoch: 7 | Batch Status: 35200/60000 (59%) | Loss: 0.104647\n",
      "Train Epoch: 7 | Batch Status: 35840/60000 (60%) | Loss: 0.038338\n",
      "Train Epoch: 7 | Batch Status: 36480/60000 (61%) | Loss: 0.083431\n",
      "Train Epoch: 7 | Batch Status: 37120/60000 (62%) | Loss: 0.081416\n",
      "Train Epoch: 7 | Batch Status: 37760/60000 (63%) | Loss: 0.300102\n",
      "Train Epoch: 7 | Batch Status: 38400/60000 (64%) | Loss: 0.137861\n",
      "Train Epoch: 7 | Batch Status: 39040/60000 (65%) | Loss: 0.093776\n",
      "Train Epoch: 7 | Batch Status: 39680/60000 (66%) | Loss: 0.053971\n",
      "Train Epoch: 7 | Batch Status: 40320/60000 (67%) | Loss: 0.098133\n",
      "Train Epoch: 7 | Batch Status: 40960/60000 (68%) | Loss: 0.123490\n",
      "Train Epoch: 7 | Batch Status: 41600/60000 (69%) | Loss: 0.079618\n",
      "Train Epoch: 7 | Batch Status: 42240/60000 (70%) | Loss: 0.098320\n",
      "Train Epoch: 7 | Batch Status: 42880/60000 (71%) | Loss: 0.091942\n",
      "Train Epoch: 7 | Batch Status: 43520/60000 (72%) | Loss: 0.261528\n",
      "Train Epoch: 7 | Batch Status: 44160/60000 (74%) | Loss: 0.129744\n",
      "Train Epoch: 7 | Batch Status: 44800/60000 (75%) | Loss: 0.243908\n",
      "Train Epoch: 7 | Batch Status: 45440/60000 (76%) | Loss: 0.115751\n",
      "Train Epoch: 7 | Batch Status: 46080/60000 (77%) | Loss: 0.056708\n",
      "Train Epoch: 7 | Batch Status: 46720/60000 (78%) | Loss: 0.106033\n",
      "Train Epoch: 7 | Batch Status: 47360/60000 (79%) | Loss: 0.076763\n",
      "Train Epoch: 7 | Batch Status: 48000/60000 (80%) | Loss: 0.037098\n",
      "Train Epoch: 7 | Batch Status: 48640/60000 (81%) | Loss: 0.175119\n",
      "Train Epoch: 7 | Batch Status: 49280/60000 (82%) | Loss: 0.197849\n",
      "Train Epoch: 7 | Batch Status: 49920/60000 (83%) | Loss: 0.103074\n",
      "Train Epoch: 7 | Batch Status: 50560/60000 (84%) | Loss: 0.066956\n",
      "Train Epoch: 7 | Batch Status: 51200/60000 (85%) | Loss: 0.227707\n",
      "Train Epoch: 7 | Batch Status: 51840/60000 (86%) | Loss: 0.134623\n",
      "Train Epoch: 7 | Batch Status: 52480/60000 (87%) | Loss: 0.213571\n",
      "Train Epoch: 7 | Batch Status: 53120/60000 (88%) | Loss: 0.160787\n",
      "Train Epoch: 7 | Batch Status: 53760/60000 (90%) | Loss: 0.126247\n",
      "Train Epoch: 7 | Batch Status: 54400/60000 (91%) | Loss: 0.100164\n",
      "Train Epoch: 7 | Batch Status: 55040/60000 (92%) | Loss: 0.097624\n",
      "Train Epoch: 7 | Batch Status: 55680/60000 (93%) | Loss: 0.082674\n",
      "Train Epoch: 7 | Batch Status: 56320/60000 (94%) | Loss: 0.098566\n",
      "Train Epoch: 7 | Batch Status: 56960/60000 (95%) | Loss: 0.090056\n",
      "Train Epoch: 7 | Batch Status: 57600/60000 (96%) | Loss: 0.110152\n",
      "Train Epoch: 7 | Batch Status: 58240/60000 (97%) | Loss: 0.074710\n",
      "Train Epoch: 7 | Batch Status: 58880/60000 (98%) | Loss: 0.139939\n",
      "Train Epoch: 7 | Batch Status: 59520/60000 (99%) | Loss: 0.098392\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0019, Accuracy: 9643/10000 (96%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 8 | Batch Status: 0/60000 (0%) | Loss: 0.094071\n",
      "Train Epoch: 8 | Batch Status: 640/60000 (1%) | Loss: 0.185752\n",
      "Train Epoch: 8 | Batch Status: 1280/60000 (2%) | Loss: 0.048930\n",
      "Train Epoch: 8 | Batch Status: 1920/60000 (3%) | Loss: 0.060183\n",
      "Train Epoch: 8 | Batch Status: 2560/60000 (4%) | Loss: 0.140532\n",
      "Train Epoch: 8 | Batch Status: 3200/60000 (5%) | Loss: 0.067830\n",
      "Train Epoch: 8 | Batch Status: 3840/60000 (6%) | Loss: 0.058288\n",
      "Train Epoch: 8 | Batch Status: 4480/60000 (7%) | Loss: 0.072371\n",
      "Train Epoch: 8 | Batch Status: 5120/60000 (9%) | Loss: 0.158538\n",
      "Train Epoch: 8 | Batch Status: 5760/60000 (10%) | Loss: 0.192062\n",
      "Train Epoch: 8 | Batch Status: 6400/60000 (11%) | Loss: 0.104032\n",
      "Train Epoch: 8 | Batch Status: 7040/60000 (12%) | Loss: 0.070557\n",
      "Train Epoch: 8 | Batch Status: 7680/60000 (13%) | Loss: 0.158396\n",
      "Train Epoch: 8 | Batch Status: 8320/60000 (14%) | Loss: 0.127917\n",
      "Train Epoch: 8 | Batch Status: 8960/60000 (15%) | Loss: 0.037260\n",
      "Train Epoch: 8 | Batch Status: 9600/60000 (16%) | Loss: 0.294987\n",
      "Train Epoch: 8 | Batch Status: 10240/60000 (17%) | Loss: 0.147835\n",
      "Train Epoch: 8 | Batch Status: 10880/60000 (18%) | Loss: 0.183022\n",
      "Train Epoch: 8 | Batch Status: 11520/60000 (19%) | Loss: 0.118118\n",
      "Train Epoch: 8 | Batch Status: 12160/60000 (20%) | Loss: 0.071719\n",
      "Train Epoch: 8 | Batch Status: 12800/60000 (21%) | Loss: 0.111978\n",
      "Train Epoch: 8 | Batch Status: 13440/60000 (22%) | Loss: 0.200494\n",
      "Train Epoch: 8 | Batch Status: 14080/60000 (23%) | Loss: 0.161721\n",
      "Train Epoch: 8 | Batch Status: 14720/60000 (25%) | Loss: 0.233581\n",
      "Train Epoch: 8 | Batch Status: 15360/60000 (26%) | Loss: 0.056554\n",
      "Train Epoch: 8 | Batch Status: 16000/60000 (27%) | Loss: 0.136287\n",
      "Train Epoch: 8 | Batch Status: 16640/60000 (28%) | Loss: 0.128141\n",
      "Train Epoch: 8 | Batch Status: 17280/60000 (29%) | Loss: 0.088452\n",
      "Train Epoch: 8 | Batch Status: 17920/60000 (30%) | Loss: 0.075382\n",
      "Train Epoch: 8 | Batch Status: 18560/60000 (31%) | Loss: 0.123522\n",
      "Train Epoch: 8 | Batch Status: 19200/60000 (32%) | Loss: 0.035731\n",
      "Train Epoch: 8 | Batch Status: 19840/60000 (33%) | Loss: 0.166656\n",
      "Train Epoch: 8 | Batch Status: 20480/60000 (34%) | Loss: 0.086404\n",
      "Train Epoch: 8 | Batch Status: 21120/60000 (35%) | Loss: 0.054569\n",
      "Train Epoch: 8 | Batch Status: 21760/60000 (36%) | Loss: 0.092660\n",
      "Train Epoch: 8 | Batch Status: 22400/60000 (37%) | Loss: 0.111451\n",
      "Train Epoch: 8 | Batch Status: 23040/60000 (38%) | Loss: 0.044569\n",
      "Train Epoch: 8 | Batch Status: 23680/60000 (39%) | Loss: 0.065335\n",
      "Train Epoch: 8 | Batch Status: 24320/60000 (41%) | Loss: 0.080994\n",
      "Train Epoch: 8 | Batch Status: 24960/60000 (42%) | Loss: 0.381245\n",
      "Train Epoch: 8 | Batch Status: 25600/60000 (43%) | Loss: 0.088399\n",
      "Train Epoch: 8 | Batch Status: 26240/60000 (44%) | Loss: 0.159030\n",
      "Train Epoch: 8 | Batch Status: 26880/60000 (45%) | Loss: 0.048350\n",
      "Train Epoch: 8 | Batch Status: 27520/60000 (46%) | Loss: 0.130032\n",
      "Train Epoch: 8 | Batch Status: 28160/60000 (47%) | Loss: 0.143530\n",
      "Train Epoch: 8 | Batch Status: 28800/60000 (48%) | Loss: 0.256561\n",
      "Train Epoch: 8 | Batch Status: 29440/60000 (49%) | Loss: 0.055530\n",
      "Train Epoch: 8 | Batch Status: 30080/60000 (50%) | Loss: 0.102940\n",
      "Train Epoch: 8 | Batch Status: 30720/60000 (51%) | Loss: 0.059204\n",
      "Train Epoch: 8 | Batch Status: 31360/60000 (52%) | Loss: 0.060937\n",
      "Train Epoch: 8 | Batch Status: 32000/60000 (53%) | Loss: 0.140786\n",
      "Train Epoch: 8 | Batch Status: 32640/60000 (54%) | Loss: 0.272465\n",
      "Train Epoch: 8 | Batch Status: 33280/60000 (55%) | Loss: 0.050367\n",
      "Train Epoch: 8 | Batch Status: 33920/60000 (57%) | Loss: 0.106549\n",
      "Train Epoch: 8 | Batch Status: 34560/60000 (58%) | Loss: 0.091390\n",
      "Train Epoch: 8 | Batch Status: 35200/60000 (59%) | Loss: 0.170808\n",
      "Train Epoch: 8 | Batch Status: 35840/60000 (60%) | Loss: 0.222559\n",
      "Train Epoch: 8 | Batch Status: 36480/60000 (61%) | Loss: 0.206213\n",
      "Train Epoch: 8 | Batch Status: 37120/60000 (62%) | Loss: 0.052388\n",
      "Train Epoch: 8 | Batch Status: 37760/60000 (63%) | Loss: 0.152807\n",
      "Train Epoch: 8 | Batch Status: 38400/60000 (64%) | Loss: 0.057423\n",
      "Train Epoch: 8 | Batch Status: 39040/60000 (65%) | Loss: 0.056394\n",
      "Train Epoch: 8 | Batch Status: 39680/60000 (66%) | Loss: 0.051700\n",
      "Train Epoch: 8 | Batch Status: 40320/60000 (67%) | Loss: 0.082272\n",
      "Train Epoch: 8 | Batch Status: 40960/60000 (68%) | Loss: 0.118238\n",
      "Train Epoch: 8 | Batch Status: 41600/60000 (69%) | Loss: 0.032926\n",
      "Train Epoch: 8 | Batch Status: 42240/60000 (70%) | Loss: 0.042067\n",
      "Train Epoch: 8 | Batch Status: 42880/60000 (71%) | Loss: 0.082766\n",
      "Train Epoch: 8 | Batch Status: 43520/60000 (72%) | Loss: 0.067038\n",
      "Train Epoch: 8 | Batch Status: 44160/60000 (74%) | Loss: 0.094564\n",
      "Train Epoch: 8 | Batch Status: 44800/60000 (75%) | Loss: 0.082931\n",
      "Train Epoch: 8 | Batch Status: 45440/60000 (76%) | Loss: 0.088628\n",
      "Train Epoch: 8 | Batch Status: 46080/60000 (77%) | Loss: 0.107291\n",
      "Train Epoch: 8 | Batch Status: 46720/60000 (78%) | Loss: 0.069653\n",
      "Train Epoch: 8 | Batch Status: 47360/60000 (79%) | Loss: 0.121498\n",
      "Train Epoch: 8 | Batch Status: 48000/60000 (80%) | Loss: 0.111054\n",
      "Train Epoch: 8 | Batch Status: 48640/60000 (81%) | Loss: 0.064430\n",
      "Train Epoch: 8 | Batch Status: 49280/60000 (82%) | Loss: 0.055796\n",
      "Train Epoch: 8 | Batch Status: 49920/60000 (83%) | Loss: 0.074901\n",
      "Train Epoch: 8 | Batch Status: 50560/60000 (84%) | Loss: 0.115949\n",
      "Train Epoch: 8 | Batch Status: 51200/60000 (85%) | Loss: 0.081522\n",
      "Train Epoch: 8 | Batch Status: 51840/60000 (86%) | Loss: 0.244252\n",
      "Train Epoch: 8 | Batch Status: 52480/60000 (87%) | Loss: 0.038641\n",
      "Train Epoch: 8 | Batch Status: 53120/60000 (88%) | Loss: 0.151320\n",
      "Train Epoch: 8 | Batch Status: 53760/60000 (90%) | Loss: 0.040777\n",
      "Train Epoch: 8 | Batch Status: 54400/60000 (91%) | Loss: 0.062089\n",
      "Train Epoch: 8 | Batch Status: 55040/60000 (92%) | Loss: 0.054442\n",
      "Train Epoch: 8 | Batch Status: 55680/60000 (93%) | Loss: 0.104090\n",
      "Train Epoch: 8 | Batch Status: 56320/60000 (94%) | Loss: 0.069658\n",
      "Train Epoch: 8 | Batch Status: 56960/60000 (95%) | Loss: 0.111224\n",
      "Train Epoch: 8 | Batch Status: 57600/60000 (96%) | Loss: 0.036287\n",
      "Train Epoch: 8 | Batch Status: 58240/60000 (97%) | Loss: 0.057134\n",
      "Train Epoch: 8 | Batch Status: 58880/60000 (98%) | Loss: 0.076444\n",
      "Train Epoch: 8 | Batch Status: 59520/60000 (99%) | Loss: 0.051015\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0018, Accuracy: 9653/10000 (97%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 9 | Batch Status: 0/60000 (0%) | Loss: 0.128517\n",
      "Train Epoch: 9 | Batch Status: 640/60000 (1%) | Loss: 0.120032\n",
      "Train Epoch: 9 | Batch Status: 1280/60000 (2%) | Loss: 0.040485\n",
      "Train Epoch: 9 | Batch Status: 1920/60000 (3%) | Loss: 0.115350\n",
      "Train Epoch: 9 | Batch Status: 2560/60000 (4%) | Loss: 0.042673\n",
      "Train Epoch: 9 | Batch Status: 3200/60000 (5%) | Loss: 0.102057\n",
      "Train Epoch: 9 | Batch Status: 3840/60000 (6%) | Loss: 0.258852\n",
      "Train Epoch: 9 | Batch Status: 4480/60000 (7%) | Loss: 0.102235\n",
      "Train Epoch: 9 | Batch Status: 5120/60000 (9%) | Loss: 0.111239\n",
      "Train Epoch: 9 | Batch Status: 5760/60000 (10%) | Loss: 0.058075\n",
      "Train Epoch: 9 | Batch Status: 6400/60000 (11%) | Loss: 0.105964\n",
      "Train Epoch: 9 | Batch Status: 7040/60000 (12%) | Loss: 0.060986\n",
      "Train Epoch: 9 | Batch Status: 7680/60000 (13%) | Loss: 0.024465\n",
      "Train Epoch: 9 | Batch Status: 8320/60000 (14%) | Loss: 0.045584\n",
      "Train Epoch: 9 | Batch Status: 8960/60000 (15%) | Loss: 0.096003\n",
      "Train Epoch: 9 | Batch Status: 9600/60000 (16%) | Loss: 0.293659\n",
      "Train Epoch: 9 | Batch Status: 10240/60000 (17%) | Loss: 0.128357\n",
      "Train Epoch: 9 | Batch Status: 10880/60000 (18%) | Loss: 0.104413\n",
      "Train Epoch: 9 | Batch Status: 11520/60000 (19%) | Loss: 0.081765\n",
      "Train Epoch: 9 | Batch Status: 12160/60000 (20%) | Loss: 0.055806\n",
      "Train Epoch: 9 | Batch Status: 12800/60000 (21%) | Loss: 0.036768\n",
      "Train Epoch: 9 | Batch Status: 13440/60000 (22%) | Loss: 0.038200\n",
      "Train Epoch: 9 | Batch Status: 14080/60000 (23%) | Loss: 0.064740\n",
      "Train Epoch: 9 | Batch Status: 14720/60000 (25%) | Loss: 0.047092\n",
      "Train Epoch: 9 | Batch Status: 15360/60000 (26%) | Loss: 0.089654\n",
      "Train Epoch: 9 | Batch Status: 16000/60000 (27%) | Loss: 0.127153\n",
      "Train Epoch: 9 | Batch Status: 16640/60000 (28%) | Loss: 0.063330\n",
      "Train Epoch: 9 | Batch Status: 17280/60000 (29%) | Loss: 0.081318\n",
      "Train Epoch: 9 | Batch Status: 17920/60000 (30%) | Loss: 0.021834\n",
      "Train Epoch: 9 | Batch Status: 18560/60000 (31%) | Loss: 0.029312\n",
      "Train Epoch: 9 | Batch Status: 19200/60000 (32%) | Loss: 0.033778\n",
      "Train Epoch: 9 | Batch Status: 19840/60000 (33%) | Loss: 0.181881\n",
      "Train Epoch: 9 | Batch Status: 20480/60000 (34%) | Loss: 0.035647\n",
      "Train Epoch: 9 | Batch Status: 21120/60000 (35%) | Loss: 0.322474\n",
      "Train Epoch: 9 | Batch Status: 21760/60000 (36%) | Loss: 0.083632\n",
      "Train Epoch: 9 | Batch Status: 22400/60000 (37%) | Loss: 0.143748\n",
      "Train Epoch: 9 | Batch Status: 23040/60000 (38%) | Loss: 0.020061\n",
      "Train Epoch: 9 | Batch Status: 23680/60000 (39%) | Loss: 0.094445\n",
      "Train Epoch: 9 | Batch Status: 24320/60000 (41%) | Loss: 0.030396\n",
      "Train Epoch: 9 | Batch Status: 24960/60000 (42%) | Loss: 0.096766\n",
      "Train Epoch: 9 | Batch Status: 25600/60000 (43%) | Loss: 0.256052\n",
      "Train Epoch: 9 | Batch Status: 26240/60000 (44%) | Loss: 0.077864\n",
      "Train Epoch: 9 | Batch Status: 26880/60000 (45%) | Loss: 0.065418\n",
      "Train Epoch: 9 | Batch Status: 27520/60000 (46%) | Loss: 0.116499\n",
      "Train Epoch: 9 | Batch Status: 28160/60000 (47%) | Loss: 0.118570\n",
      "Train Epoch: 9 | Batch Status: 28800/60000 (48%) | Loss: 0.054154\n",
      "Train Epoch: 9 | Batch Status: 29440/60000 (49%) | Loss: 0.112656\n",
      "Train Epoch: 9 | Batch Status: 30080/60000 (50%) | Loss: 0.100639\n",
      "Train Epoch: 9 | Batch Status: 30720/60000 (51%) | Loss: 0.116608\n",
      "Train Epoch: 9 | Batch Status: 31360/60000 (52%) | Loss: 0.059619\n",
      "Train Epoch: 9 | Batch Status: 32000/60000 (53%) | Loss: 0.149354\n",
      "Train Epoch: 9 | Batch Status: 32640/60000 (54%) | Loss: 0.033353\n",
      "Train Epoch: 9 | Batch Status: 33280/60000 (55%) | Loss: 0.040384\n",
      "Train Epoch: 9 | Batch Status: 33920/60000 (57%) | Loss: 0.105913\n",
      "Train Epoch: 9 | Batch Status: 34560/60000 (58%) | Loss: 0.075398\n",
      "Train Epoch: 9 | Batch Status: 35200/60000 (59%) | Loss: 0.154305\n",
      "Train Epoch: 9 | Batch Status: 35840/60000 (60%) | Loss: 0.049980\n",
      "Train Epoch: 9 | Batch Status: 36480/60000 (61%) | Loss: 0.124137\n",
      "Train Epoch: 9 | Batch Status: 37120/60000 (62%) | Loss: 0.096541\n",
      "Train Epoch: 9 | Batch Status: 37760/60000 (63%) | Loss: 0.015974\n",
      "Train Epoch: 9 | Batch Status: 38400/60000 (64%) | Loss: 0.038725\n",
      "Train Epoch: 9 | Batch Status: 39040/60000 (65%) | Loss: 0.034074\n",
      "Train Epoch: 9 | Batch Status: 39680/60000 (66%) | Loss: 0.094738\n",
      "Train Epoch: 9 | Batch Status: 40320/60000 (67%) | Loss: 0.056646\n",
      "Train Epoch: 9 | Batch Status: 40960/60000 (68%) | Loss: 0.124664\n",
      "Train Epoch: 9 | Batch Status: 41600/60000 (69%) | Loss: 0.068334\n",
      "Train Epoch: 9 | Batch Status: 42240/60000 (70%) | Loss: 0.057373\n",
      "Train Epoch: 9 | Batch Status: 42880/60000 (71%) | Loss: 0.087118\n",
      "Train Epoch: 9 | Batch Status: 43520/60000 (72%) | Loss: 0.047678\n",
      "Train Epoch: 9 | Batch Status: 44160/60000 (74%) | Loss: 0.074621\n",
      "Train Epoch: 9 | Batch Status: 44800/60000 (75%) | Loss: 0.068639\n",
      "Train Epoch: 9 | Batch Status: 45440/60000 (76%) | Loss: 0.133800\n",
      "Train Epoch: 9 | Batch Status: 46080/60000 (77%) | Loss: 0.125834\n",
      "Train Epoch: 9 | Batch Status: 46720/60000 (78%) | Loss: 0.102238\n",
      "Train Epoch: 9 | Batch Status: 47360/60000 (79%) | Loss: 0.138326\n",
      "Train Epoch: 9 | Batch Status: 48000/60000 (80%) | Loss: 0.043263\n",
      "Train Epoch: 9 | Batch Status: 48640/60000 (81%) | Loss: 0.111524\n",
      "Train Epoch: 9 | Batch Status: 49280/60000 (82%) | Loss: 0.070096\n",
      "Train Epoch: 9 | Batch Status: 49920/60000 (83%) | Loss: 0.055992\n",
      "Train Epoch: 9 | Batch Status: 50560/60000 (84%) | Loss: 0.201828\n",
      "Train Epoch: 9 | Batch Status: 51200/60000 (85%) | Loss: 0.030308\n",
      "Train Epoch: 9 | Batch Status: 51840/60000 (86%) | Loss: 0.046872\n",
      "Train Epoch: 9 | Batch Status: 52480/60000 (87%) | Loss: 0.030295\n",
      "Train Epoch: 9 | Batch Status: 53120/60000 (88%) | Loss: 0.068809\n",
      "Train Epoch: 9 | Batch Status: 53760/60000 (90%) | Loss: 0.087813\n",
      "Train Epoch: 9 | Batch Status: 54400/60000 (91%) | Loss: 0.110657\n",
      "Train Epoch: 9 | Batch Status: 55040/60000 (92%) | Loss: 0.038420\n",
      "Train Epoch: 9 | Batch Status: 55680/60000 (93%) | Loss: 0.089199\n",
      "Train Epoch: 9 | Batch Status: 56320/60000 (94%) | Loss: 0.115221\n",
      "Train Epoch: 9 | Batch Status: 56960/60000 (95%) | Loss: 0.225722\n",
      "Train Epoch: 9 | Batch Status: 57600/60000 (96%) | Loss: 0.077902\n",
      "Train Epoch: 9 | Batch Status: 58240/60000 (97%) | Loss: 0.032927\n",
      "Train Epoch: 9 | Batch Status: 58880/60000 (98%) | Loss: 0.045725\n",
      "Train Epoch: 9 | Batch Status: 59520/60000 (99%) | Loss: 0.084765\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0016, Accuracy: 9679/10000 (97%)\n",
      "Testing time: 0m 7s\n",
      "Total Time: 1m 2s\n",
      "Model was trained on cpu!\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "from torch import nn, optim, cuda\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).item()\n",
    "        # get the index of the max\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    since = time.time()\n",
    "    for epoch in range(1, 10):\n",
    "        epoch_start = time.time()\n",
    "        train(epoch)\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
    "        test()\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
    "\n",
    "    m, s = divmod(time.time() - since, 60)\n",
    "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
