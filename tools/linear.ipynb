{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596663374518",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Review\n",
    "\n",
    "This note looks at Linear Algebra from ML and data analytics point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Revisit Matrix \n",
    "\n",
    "One aspect of \"fresh look\" is, not to see matrix vector multiplication as dot product. Instead, see this as a *linear combination* of column vectors.\n",
    "\n",
    "\\begin{align} Ax &= \\begin{bmatrix}\n",
    "                        2 & 5 & 7 \\\\\n",
    "                        3 & 6 & 9 \\\\\n",
    "                        4 & 7 & 11 \\\\\n",
    "                        \\end{bmatrix} \n",
    "                    \n",
    "                        \\begin{bmatrix}\n",
    "                        x_1 \\\\\n",
    "                        x_2 \\\\\n",
    "                        x_3 \\\\\n",
    "                        \\end{bmatrix} \\\\\n",
    "                &= x_1 \\begin{bmatrix}\n",
    "                    2 \\\\\n",
    "                    3 \\\\\n",
    "                    4 \\\\\n",
    "                    \\end{bmatrix} + \\quad\n",
    "                    x_2 \\begin{bmatrix}\n",
    "                    5 \\\\\n",
    "                    6 \\\\\n",
    "                    7 \\\\\n",
    "                    \\end{bmatrix} + \\quad\n",
    "                    x_3 \\begin{bmatrix}\n",
    "                    7 \\\\\n",
    "                    9 \\\\\n",
    "                    11 \\\\\n",
    "                    \\end{bmatrix}\n",
    "\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis vector and span\n",
    "\n",
    "Matrices have column space and row space. A column space is what can be represented by *ALL* the linear combination of basis vector of that space. It is also called a **span**. A row space, on the other hand, can be see as the column space of the transposed matrix.\n",
    "\n",
    "If the vectors are not basis vector, then the span can be a just point (zero vectors); or a line if two vectors are dependent, or a plane if two out of three are dependent.\n",
    "\n",
    "\n",
    "What is the vector basis? The basis of a vector space is a set of **linearly independent** vectors that **span* the full space.\n",
    "\n",
    "Using above $A$ as an example:\n",
    "\n",
    "\\begin{bmatrix} 2 \\\\ 3 \\\\ 4 \\\\ \\end{bmatrix} and \\begin{bmatrix} 5 \\\\ 6\\\\ 7 \\\\ \\end{bmatrix} are column vector basis. The third one is not, because it is a linear combination of the first two (sum of both).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices as Linear Transformation\n",
    "\n",
    "A transformation is a function, take something in, spit something out. A linear transformation, visually, is to (1) keep origin in the smae place (2) keep all grid lines straight (no curves) and evenly spaced.\n",
    "\n",
    "\n",
    "One easy example of transformation: rotate the 2-D plane.\n",
    "\n",
    "Given a vector $v$, to deduce how it is transformed, all we need to know is how the basis vectors are transformed, and the vector $v$ will follow the same relationship in the new transformed coordinate space.\n",
    "\n",
    "The following figure gives an example of linear transform in 2-D space: $\\hat{i}$ and $\\hat{j}$ are basis vectors, and its position in the new transformed plane. Given any vector $\\begin{bmatrix} x \\\\ y \\end{bmatrix}$, we can calculate its new position using the $\\hat{i}$ and $\\hat{j}$ information.\n",
    "\n",
    "\n",
    "![linear transform](figs/linear1.png)\n",
    "\n",
    "\n",
    "More generally, in 2x2 matrix, this can be summarized as:\n",
    "\n",
    "![linear transform](figs/linear2.png)\n",
    "\n",
    "But the idea is the same for higher dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "\n",
    "The transformation discussion give us a interesting view on what is matrix multiplication:\n",
    "\n",
    "$$M_1 M_2 M_3 \\ldots $$\n",
    "\n",
    "are essentially saying apply $M_3$ transformation first, then $M_2$, then $M_1$, from right to left. This should remind us the composition of functions $f(g(x))$. We are here composing matrices by finding out the product of matrices.\n",
    " \n",
    "\n",
    "Here, instead of doing regular dot product, which is:\n",
    "$$\\text{rows} \\times \\text{columns}$$ We look at the other ways around, which would be \n",
    "$$\\text{columns} \\times \\text{rows}$$\n",
    "\n",
    "The following code demos both ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[12, 26, 40],\n       [15, 33, 51],\n       [23, 53, 83]])"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import numpy as np \n",
    "A = np.array([\n",
    "    [2, 5],\n",
    "    [3, 6],\n",
    "    [7, 8]\n",
    "])\n",
    "B = np.array([\n",
    "    [1,3,5],\n",
    "    [2,4,6]\n",
    "])\n",
    "np.dot(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[12, 26, 40],\n       [15, 33, 51],\n       [18, 40, 62]])"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "A_col_1 = np.array([\n",
    "    [2],\n",
    "    [3],\n",
    "    [4]\n",
    "])\n",
    "B_row_1 = np.array([\n",
    "    [1,3,5]\n",
    "])\n",
    "\n",
    "A_col_2 = np.array([\n",
    "    [5],\n",
    "    [6],\n",
    "    [7]\n",
    "])\n",
    "\n",
    "B_row_2 = np.array([\n",
    "    [2,4,6]\n",
    "])\n",
    "\n",
    "np.dot(A_col_1, B_row_1) + np.dot(A_col_2, B_row_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinant \n",
    "\n",
    "Visually (in 2-D) - determinant of a matrix is the scaling factor of an area transforming from one to another. Say, if the $\\det(A)$ is 3, that means, an area will be multiplied by 3 in the new transformed space. And if determinant is zero, that means tranformation will squish it to a line or a point, thus the area will be zero.\n",
    "\n",
    "When determinant is negative, the *orientation of space* is inverted.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-2.0000000000000004\n-9.51619735392994e-16\n"
    }
   ],
   "source": [
    "# 2 x 2 determinant is easy, but 3x3 and up is not\n",
    "import numpy as np\n",
    "a = np.array([[1,2], [3,4]]) \n",
    "print(np.linalg.det(a))\n",
    "b = np.array([\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9],\n",
    "    ])\n",
    "print(np.linalg.det(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear System of Equations\n",
    "\n",
    "This can be written as $A\\vec{x} = \\vec{v}$.\n",
    "$A$ is coefficient matrix, $\\vec{x}$ is unknown variable vectors, $\\vec{v}$ is usually a constant (known) vector. This is what we are familiar with.\n",
    "\n",
    "From transformation point of view, however, this can be seen as: we have a transformaion matrix $A$, and we know after the transformation, a vector will land as $\\vec{v}$ in new space. What is the original vector $\\vec{x}$ in previous space?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Transform and Inverse Matrix\n",
    "\n",
    "If we have been thinking about forward transformation: given a vector, transform it with $A$, what is the new vector. Now we should think in terms of \"inverse transformation\": previously we do clock wise rotatin, now it is counter-clock rotation, forward-shearing now is backward-shearing etc. Through \"inverse transformation\", we can start from the new vector $\\vec{v}$, and figure out what is the original vector $\\vec{x}$. \n",
    "\n",
    "This inverse transform bring us the concept of \"inverse matrix\".\n",
    "\n",
    "\n",
    "$$A^{-1} A = A A^{-1} = I$$\n",
    "\n",
    "This inverse property of $A$ is: you start with transformation $A$, and you do the inverse $A^{-1}$, you are back to where you started. $I$ is a matrix, and it is a transformation as well. However, $I$ is a transformation that does nothing. $I$ is the identity matrix.\n",
    "\n",
    "* Matrix $AB$'s inverse is $B^{-1}A^{-1}$ in that order, since $(AB)(B^{-1}A^{-1}) = I$\n",
    "\n",
    "\n",
    "### Using inverse to solve linear system of equations\n",
    "\n",
    "Once you have this inverse, you can:\n",
    "\n",
    "$$ A^{-1} A \\vec{x} = A^{-1} \\vec{v}$$\n",
    "$$ \\vec{x} = A^{-1} \\vec{v}$$\n",
    "\n",
    "### the condition of inverse exists\n",
    "\n",
    "is that $$\\det(A) \\neq 0$$\n",
    "\n",
    "If $\\det(A) = 0$, that means transformation squish to smaller dimension (maybe a point, or a line, or a plane), there is no inverse. Geometrically, you can not \"undo\" from a line to a plane etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank and Column Space\n",
    "\n",
    "* If a transformation squishes everything to a line, we say, the transformation has **rank** of 1.\n",
    "* If a transformation make everything land on a plane, we say, the transformation has **rank** of 2.\n",
    "\n",
    "So **rank** is defined as: \"the number of dimension in the output\"\n",
    "\n",
    "For example, in the case of 2x2 matrix, rank = 2 is the best it can be.\n",
    "that is, after the transformation, the $\\hat{i}$ and $\\hat{j}$ continue to fill up the full space, not squished to a point or a line.\n",
    "\n",
    "\n",
    "The set of all possible output $A\\vec{x}$ is also called the **column space** of the matrix $A$.\n",
    "Since each column of $A$ is telling that where the basis vector will land after the transformation.\n",
    "\n",
    "\n",
    "\n",
    "* With that column space in mind, if we pick a random vector $x$, then $Ax$ will in the column space of $A$ - as $Ax$ gives you just one linear combination of columns.\n",
    "\n",
    "* $ABCx = A(BCx) = Ay$, which means $ABCx$ is also in the column space of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Rank\n",
    "\n",
    "In the \"Revisit matrix\", we write the $A\\vec{x}$ as linear combination of columns, thus,\n",
    "a **column** space can be regarded as the **span** of columns in a matrix $A$. Rank can be more precisely defined as the number of dimensions in the **column rank**.\n",
    "\n",
    "If the rank is as high as it can, which equals to the number of columns, then it is known as the **full rank**. \n",
    "\n",
    "\n",
    "The rank of $A$ is the number of vector basis: $$C(A) = 2$$\n",
    "\n",
    "Another property is: column rank equals to the row rank.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null space/Kernel\n",
    "\n",
    "The zero vector (**origin**) will always be included in the column space.\n",
    "For a **full rank** transformation, the only vector that lands on the origin is the zero vector.\n",
    "\n",
    "But, if you don't have a full rank, the transformation will squish the space, you can have bunch of vectors landing on zero:\n",
    "* 2-d transformation, you can have a line of vectors ending on origin\n",
    "* 3-d transformation, to (1) a plane or (2) a line, you can have either (1) a line or (2) a full plane of vectors ending on orgin. \n",
    "\n",
    "\n",
    "This set of vectors that lands on the orgin is called **Null Space** or the **Kernel** of the matrix.\n",
    "\n",
    "In the context of linear system of equations, if $\\vec{v}$ happen to be zero:\n",
    "$$A\\vec{x} = \\vec{v} = 0$$\n",
    "\n",
    "\n",
    "The null space gives you **all the possible solutions** for the equations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## None-square matrices\n",
    "\n",
    "All previous discussion has been on squared matrix. 2x2, 3x3, including transformation. So there is a 1-to-1 mapping of basis vector from one space to another.\n",
    "\n",
    "For non-squared matrices, we can explain similarly, but it is geometrically messy, see 3BLUE1BROWN chapter 8.\n",
    "\n",
    "* For 2x3 matrix, we have 3 basis vectors, but since we only have two row, the transformation landing is on a plane.\n",
    "\n",
    "* For 3x2 matrix, we have 2 basis vectors, but the transformation landing is on a 3-d space since we have 3 rows.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "\n",
    "Dot product refers to multiplying two vectors: each pair of elements multiply, then sum it up as a single number.\n",
    "\n",
    "Dot product is *not* matrix multiplication.\n",
    "Matrix multiplication *uses* dot product.\n",
    "\n",
    "Geometrically, dot product gives you a sense of how two vectors are related: if they point to the same direction, dot product is positive; otherwise, it is negative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross product\n",
    "\n",
    "A 2-d example: $\\vec{v} = \\begin{bmatrix} -3\\\\ 1\\end{bmatrix}$ and $\\vec{w} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$.\n",
    "\n",
    "$$ \\vec{v} \\times \\vec{w} = \\det\\Big( \\begin{bmatrix} -3 & 2 \\\\ 1 & 1 \\end{bmatrix} \\Big) = -3 \\cdot 1 - 2 \\cdot 1 = -5 $$\n",
    "\n",
    "Geometrically, the cross product represents the area of parallelogram.\n",
    "\n",
    "![](figs/cross-product.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Matrix factorization\n",
    "\n",
    "\\begin{align} A &= \\begin{bmatrix}\n",
    "                        2 & 5 & 7 \\\\\n",
    "                        3 & 6 & 9 \\\\\n",
    "                        4 & 7 & 11 \\\\\n",
    "                        \\end{bmatrix} \n",
    "                    \n",
    "    &= \\begin{bmatrix}\n",
    "                        2 & 5 \\\\\n",
    "                        3 & 6  \\\\\n",
    "                        4 & 7 \\\\\n",
    "                        \\end{bmatrix} \n",
    "        \\begin{bmatrix}\n",
    "        1 & 0 & 1 \\\\\n",
    "        0 & 1 & 1  \\\\\n",
    "        \\end{bmatrix} \\\\\n",
    "    &= C R \n",
    "\\end{align}\n",
    "\n",
    "Make sense of this decomposition: \n",
    "* to have $[2, 3, 4]$ from $A$, we need 1 column basis \n",
    "$[2, 3, 4]$ and zero column basis of $[5, 6, 7]$, so we end with \n",
    "$\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ \n",
    "* to have $[5, 6, 7]$ from $A$, we need zero first column basis, and one second column basis, so we have $\\begin{bmatrix} 0 \\\\  1\\end{bmatrix}$\n",
    "* to have $[7, 9, 11]$, we need 1 first column basis plus one second column basis, so we have $\\begin{bmatrix} 1 \\\\  1\\end{bmatrix}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}